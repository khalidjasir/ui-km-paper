{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalidjasir/ui-km-paper/blob/main/KM_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c382e3",
      "metadata": {
        "id": "d9c382e3"
      },
      "source": [
        "## Install necessary library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0150f859-7fdf-4ec8-80d3-5246d0ba3d87",
      "metadata": {
        "id": "0150f859-7fdf-4ec8-80d3-5246d0ba3d87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0441871c-d40d-4d23-d35f-182e3f26c249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (2025.2)\n",
            "Collecting swifter\n",
            "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.11/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2024.12.1)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.11/dist-packages (from swifter) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.2.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.7.0)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.1.21)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (18.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16505 sha256=89e8838abfac3933d2e80cbfa9587d35fcfb5d3e0d12532e4a5d27dd66ce3574\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/7f/bd/9bed48f078f3ee1fa75e0b29b6e0335ce1cb03a38d3443b3a3\n",
            "Successfully built swifter\n",
            "Installing collected packages: swifter\n",
            "Successfully installed swifter-1.4.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting telethon\n",
            "  Downloading Telethon-1.40.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pyaes (from telethon)\n",
            "  Downloading pyaes-1.6.1.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: rsa in /usr/local/lib/python3.11/dist-packages (from telethon) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa->telethon) (0.6.1)\n",
            "Downloading Telethon-1.40.0-py3-none-any.whl (722 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.0/722.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyaes\n",
            "  Building wheel for pyaes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaes: filename=pyaes-1.6.1-py3-none-any.whl size=26347 sha256=64b171e9ec993649c63353f9298a474f5c28e87d068a304911a38fb8c86397e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/52/33/010d0843550bffb6a591b11629070ae140c0ad4f53e68a3bd3\n",
            "Successfully built pyaes\n",
            "Installing collected packages: pyaes, telethon\n",
            "Successfully installed pyaes-1.6.1 telethon-1.40.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "Collecting indonlp\n",
            "  Downloading indoNLP-0.3.4-py3-none-any.whl.metadata (3.4 kB)\n",
            "Downloading indoNLP-0.3.4-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: indonlp\n",
            "Successfully installed indonlp-0.3.4\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313499 sha256=b46569c86e0d435ad0ca69f6db5196f5513683488d0c3be926dcc99970756bca\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ],
      "source": [
        "# Core\n",
        "%pip install pandas\n",
        "\n",
        "# Utility\n",
        "%pip install python-dotenv\n",
        "%pip install pytz\n",
        "%pip install swifter\n",
        "%pip install tqdm\n",
        "%pip install ipywidgets\n",
        "%pip install openpyxl\n",
        "\n",
        "# Telegram Scraper\n",
        "%pip install telethon\n",
        "\n",
        "# NLP\n",
        "%pip install emoji\n",
        "%pip install indonlp\n",
        "%pip install fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "pQi5SbcG0lkE"
      },
      "id": "pQi5SbcG0lkE"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b4Z4eJeJ0lNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f4cc3a-0d3b-4294-874c-f5eff9678931"
      },
      "id": "b4Z4eJeJ0lNX",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to detect environment"
      ],
      "metadata": {
        "id": "Z_U3AOX9qmkY"
      },
      "id": "Z_U3AOX9qmkY"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "def detect_environment():\n",
        "  try:\n",
        "    import google.colab\n",
        "    return 'colab'\n",
        "  except ImportError:\n",
        "    pass\n",
        "\n",
        "  if 'CONDA_PREFIX' in os.environ or os.path.exists(os.path.join(sys.prefix, 'conda-meta')):\n",
        "    return 'conda'\n",
        "\n",
        "  return 'local'\n",
        "\n",
        "env = detect_environment()"
      ],
      "metadata": {
        "id": "uzktgq0uqnDl"
      },
      "id": "uzktgq0uqnDl",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Key"
      ],
      "metadata": {
        "id": "-6CQw26_vv1E"
      },
      "id": "-6CQw26_vv1E"
    },
    {
      "cell_type": "code",
      "source": [
        "api_id = ''\n",
        "api_hash = ''\n",
        "\n",
        "if env == 'colab':\n",
        "  from google.colab import userdata\n",
        "  api_id = userdata.get('api_id')\n",
        "  api_hash = userdata.get('api_hash')\n",
        "elif env == 'conda':\n",
        "  from dotenv import load_dotenv\n",
        "  load_dotenv()\n",
        "  api_id = os.getenv('api_id')\n",
        "  api_hash = os.getenv('api_hash')\n",
        "else:\n",
        "  print('Unable to detect suitable environment!')"
      ],
      "metadata": {
        "id": "XdJAZAwZvwIK"
      },
      "id": "XdJAZAwZvwIK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00581385",
      "metadata": {
        "id": "00581385"
      },
      "source": [
        "## Retrieve from telegram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74dfdc16-75fa-4582-a675-d87d16974d2c",
      "metadata": {
        "id": "74dfdc16-75fa-4582-a675-d87d16974d2c"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "from telethon import TelegramClient\n",
        "from pytz import timezone\n",
        "\n",
        "session_name = 'my_session'\n",
        "channel_input = 'https://t.me/diskusipajak'\n",
        "wib_timezone = timezone('Asia/Jakarta')\n",
        "start_date = wib_timezone.localize(datetime(2015, 1, 1))\n",
        "end_date = wib_timezone.localize(datetime(2025, 4, 30))\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "csv_filename = f'telegram_messages_{timestamp}.csv'\n",
        "batch_size = 10000\n",
        "\n",
        "async def main():\n",
        "  async with TelegramClient(session_name, api_id, api_hash) as client:\n",
        "    channel = await client.get_entity(channel_input)\n",
        "\n",
        "    buffer = []\n",
        "    total_count = 0\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "      writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "      writer.writerow([\n",
        "        'id',\n",
        "        'date',\n",
        "        'text',\n",
        "        'sender_id',\n",
        "        'chat_id',\n",
        "        'reply_to_msg_id',\n",
        "        'views',\n",
        "        'forwards',\n",
        "        'buttons',\n",
        "        'raw_text',\n",
        "        'message_link'\n",
        "      ])\n",
        "\n",
        "    async for msg in client.iter_messages(channel, offset_date=start_date, reverse=True):\n",
        "      if msg.date > end_date:\n",
        "        break\n",
        "\n",
        "      row = [\n",
        "        msg.id,\n",
        "        msg.date.astimezone(wib_timezone).strftime('%a %b %d %H:%M:%S %z %Y') if msg.date else \"\",\n",
        "        msg.text.replace('\\n', ' ').strip() if msg.text else \"\",\n",
        "        msg.sender_id if msg.sender_id else '',\n",
        "        getattr(msg, 'chat_id', getattr(msg.to_id, 'channel_id', '')),\n",
        "        msg.reply_to_msg_id if msg.reply_to_msg_id else '',\n",
        "        msg.views if msg.views is not None else '',\n",
        "        msg.forwards if msg.forwards is not None else '',\n",
        "        len(msg.buttons) if msg.buttons else 0,\n",
        "        msg.raw_text.replace('\\n', ' ').strip() if msg.raw_text else '',\n",
        "        f'https://t.me/{channel.username}/{msg.id}'\n",
        "      ]\n",
        "\n",
        "      buffer.append(row)\n",
        "      total_count += 1\n",
        "\n",
        "      if total_count % batch_size == 0:\n",
        "        with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
        "          writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "          writer.writerows(buffer)\n",
        "\n",
        "        buffer.clear()\n",
        "        print(f'Written {total_count} messages. Sleeping for 10 seconds...')\n",
        "        time.sleep(10)\n",
        "\n",
        "    if buffer:\n",
        "      with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "        writer.writerows(buffer)\n",
        "      print(f'Final batch written. Total messages: {total_count}')\n",
        "\n",
        "    print(f'Done! Messages saved to {csv_filename}.')\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move file for later processing"
      ],
      "metadata": {
        "id": "x3Mlc4ch0t3R"
      },
      "id": "x3Mlc4ch0t3R"
    },
    {
      "cell_type": "code",
      "source": [
        "if env == 'colab':\n",
        "  file_path = '/content/drive/MyDrive/telegram-data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $csv_filename $file_path\n",
        "  print(f'Successfully moved {csv_filename} to {file_path}')\n",
        "elif env == 'conda':\n",
        "  file_path = './data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $csv_filename $file_path\n",
        "  print(f'Successfully moved {csv_filename} to {file_path}')\n",
        "else:\n",
        "  print('Unable to detect suitable environment! No file moved')"
      ],
      "metadata": {
        "id": "BerT_yDW0tYc"
      },
      "id": "BerT_yDW0tYc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to detect environment"
      ],
      "metadata": {
        "id": "HhtOqJABnpz5"
      },
      "id": "HhtOqJABnpz5"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "def detect_environment():\n",
        "  try:\n",
        "    import google.colab\n",
        "    return 'colab'\n",
        "  except ImportError:\n",
        "    pass\n",
        "\n",
        "  if 'CONDA_PREFIX' in os.environ or os.path.exists(os.path.join(sys.prefix, 'conda-meta')):\n",
        "    return 'conda'\n",
        "\n",
        "  return 'local'\n",
        "\n",
        "env = detect_environment()"
      ],
      "metadata": {
        "id": "inEHphaMnsfJ"
      },
      "id": "inEHphaMnsfJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b3615095",
      "metadata": {
        "id": "b3615095"
      },
      "source": [
        "## Retrieve stored telegram messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e5e7a4",
      "metadata": {
        "id": "c0e5e7a4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = 'telegram_messages_20250519_042802.csv'\n",
        "\n",
        "dtype = {\n",
        "  'id': 'string',\n",
        "  'date': 'string',\n",
        "  'text': 'string',\n",
        "  'sender_id': 'string',\n",
        "  'chat_id': 'string',\n",
        "  'reply_to_msg_id': 'string',\n",
        "  'views': 'Int64',\n",
        "  'forwards': 'Int64',\n",
        "  'buttons': 'Int64',\n",
        "  'raw_text': 'string',\n",
        "  'message_link': 'string'\n",
        "}\n",
        "\n",
        "try:\n",
        "  if env == 'colab':\n",
        "    file_path = f'/content/drive/MyDrive/telegram-data/{filename}'\n",
        "  elif env == 'conda':\n",
        "    file_path = f'./data/{filename}'\n",
        "  else:\n",
        "    raise EnvironmentError('Unable to detect suitable environment!')\n",
        "\n",
        "  messages_df = pd.read_csv(file_path, dtype=dtype)\n",
        "  messages_df['date'] = pd.to_datetime(\n",
        "    messages_df['date'],\n",
        "    format='%a %b %d %H:%M:%S %z %Y',\n",
        "    errors='coerce'\n",
        "  )\n",
        "  messages_df['date'] = messages_df['date'].apply(\n",
        "    lambda x: x.replace(tzinfo=None) if pd.notnull(x) else x\n",
        "  )\n",
        "  print(f'Successfully read from {file_path} to dataframe')\n",
        "except FileNotFoundError:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "except Exception as e:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(messages_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze data distribution"
      ],
      "metadata": {
        "id": "yEoTrZNWPlmt"
      },
      "id": "yEoTrZNWPlmt"
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_counts = messages_df.groupby(messages_df['date'].dt.to_period('M')).size()\n",
        "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
        "\n",
        "quarterly_counts = messages_df.groupby(messages_df['date'].dt.to_period('Q')).size()\n",
        "quarterly_counts.index = quarterly_counts.index.to_timestamp()\n",
        "\n",
        "def get_semester(date):\n",
        "  if pd.isnull(date):\n",
        "    return None\n",
        "  return f'{date.year}-S1' if date.month <= 6 else f'{date.year}-S2'\n",
        "\n",
        "messages_df['semester'] = messages_df['date'].apply(get_semester)\n",
        "semesterly_counts = messages_df.groupby('semester').size()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# monthly_counts.sort_index().plot(label='Monthly')\n",
        "quarterly_counts.sort_index().plot(label='Quarterly')\n",
        "# semesterly_counts.sort_index().plot(label='Semesterly')\n",
        "\n",
        "plt.title('Message Volume Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Messages')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UJpusGEXPn_l"
      },
      "id": "UJpusGEXPn_l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "60e93023",
      "metadata": {
        "id": "60e93023"
      },
      "source": [
        "## Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7252cbd4-5782-4005-9397-620fbe6b11d3",
      "metadata": {
        "id": "7252cbd4-5782-4005-9397-620fbe6b11d3"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import emoji\n",
        "import re\n",
        "import string\n",
        "from indoNLP.preprocessing import pipeline, remove_html, remove_url, replace_slang, replace_word_elongation\n",
        "\n",
        "# Pre-compiled regex patterns\n",
        "USERNAME_RE = re.compile(r'@\\w+')\n",
        "RT_RE = re.compile(r'\\brt\\b', flags=re.IGNORECASE)\n",
        "HASHTAG_RE = re.compile(r'#')\n",
        "DIGIT_RE = re.compile(r'\\d+')\n",
        "WHITESPACE_RE = re.compile(r'\\s+')\n",
        "\n",
        "# Pre-compiled translation\n",
        "PUNCT_TRANSLATOR = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def fast_clean(text):\n",
        "  # Case folding to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Normalize Unicode (remove fancy fonts, underlines)\n",
        "  text = unicodedata.normalize('NFKD', text)\n",
        "  text = ''.join(c for c in text if not unicodedata.combining(c))\n",
        "\n",
        "  # Remove usernames\n",
        "  text = USERNAME_RE.sub('', text)\n",
        "\n",
        "  # Remove RT\n",
        "  text = RT_RE.sub('', text)\n",
        "\n",
        "  # Remove hashtag symbol but keep the word\n",
        "  text = HASHTAG_RE.sub('', text)\n",
        "\n",
        "  # Remove digits\n",
        "  text = DIGIT_RE.sub('', text)\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = text.translate(PUNCT_TRANSLATOR)\n",
        "\n",
        "  # Remove emojis\n",
        "  text = emoji.replace_emoji(text, replace='')\n",
        "\n",
        "  # Remove extra whitespace\n",
        "  text = WHITESPACE_RE.sub(' ', text).strip()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9cc6cb3",
      "metadata": {
        "id": "a9cc6cb3"
      },
      "source": [
        "## Run preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1901ff96-5c36-4386-a83a-ecfb01bdc8f5",
      "metadata": {
        "id": "1901ff96-5c36-4386-a83a-ecfb01bdc8f5"
      },
      "outputs": [],
      "source": [
        "import swifter\n",
        "\n",
        "messages_df.dropna(subset=[\n",
        "  'text',\n",
        "  'raw_text'\n",
        "], inplace=True)\n",
        "\n",
        "messages_df['fast_clean'] = messages_df['text'].swifter.apply(fast_clean)\n",
        "\n",
        "indonlp_pipeline = pipeline([\n",
        "  remove_html,\n",
        "  remove_url,\n",
        "  replace_slang,\n",
        "  replace_word_elongation\n",
        "])\n",
        "\n",
        "messages_df['basic_clean'] = messages_df['fast_clean'].swifter.apply(indonlp_pipeline)\n",
        "\n",
        "messages_df.dropna(subset=[\n",
        "  'text',\n",
        "  'raw_text',\n",
        "  'fast_clean',\n",
        "  'basic_clean'\n",
        "], inplace=True)\n",
        "\n",
        "display(messages_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8f8011",
      "metadata": {
        "id": "3e8f8011"
      },
      "source": [
        "## Filter out text that is shorter than 5 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478d01ee",
      "metadata": {
        "id": "478d01ee"
      },
      "outputs": [],
      "source": [
        "messages_df = messages_df[messages_df['basic_clean'].str.split().str.len() > 5].copy()\n",
        "\n",
        "display(messages_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze data distribution"
      ],
      "metadata": {
        "id": "YG4xK2tLRZ1s"
      },
      "id": "YG4xK2tLRZ1s"
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_counts = messages_df.groupby(messages_df['date'].dt.to_period('M')).size()\n",
        "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
        "\n",
        "quarterly_counts = messages_df.groupby(messages_df['date'].dt.to_period('Q')).size()\n",
        "quarterly_counts.index = quarterly_counts.index.to_timestamp()\n",
        "\n",
        "def get_semester(date):\n",
        "  if pd.isnull(date):\n",
        "    return None\n",
        "  return f'{date.year}-S1' if date.month <= 6 else f'{date.year}-S2'\n",
        "\n",
        "messages_df['semester'] = messages_df['date'].apply(get_semester)\n",
        "semesterly_counts = messages_df.groupby('semester').size()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# monthly_counts.sort_index().plot(label='Monthly')\n",
        "quarterly_counts.sort_index().plot(label='Quarterly')\n",
        "# semesterly_counts.sort_index().plot(label='Semesterly')\n",
        "\n",
        "plt.title('Message Volume Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Messages')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5MBSiYwBRaVw"
      },
      "id": "5MBSiYwBRaVw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving language model"
      ],
      "metadata": {
        "id": "JP9ILVEjOy0F"
      },
      "id": "JP9ILVEjOy0F"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fasttext\n",
        "import urllib.request\n",
        "\n",
        "filename = 'lid.176.bin'\n",
        "model_url = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin'\n",
        "model_path = ''\n",
        "\n",
        "try:\n",
        "  if env == 'colab':\n",
        "    model_dir = '/content/drive/MyDrive/models/fasttext'\n",
        "  elif env == 'conda':\n",
        "    model_dir = './models/fasttext'\n",
        "  else:\n",
        "    raise EnvironmentError('Unable to detect suitable environment!')\n",
        "\n",
        "  os.makedirs(model_dir, exist_ok=True)\n",
        "  model_path = os.path.join(model_dir, filename)\n",
        "\n",
        "  if not os.path.exists(model_path):\n",
        "    print('Model not found locally. Downloading...')\n",
        "    urllib.request.urlretrieve(model_url, model_path)\n",
        "    print('Download completed!')\n",
        "\n",
        "  print(f'Loading model from {model_path}...')\n",
        "  model = fasttext.load_model(model_path)\n",
        "  print('Model loaded successfully!')\n",
        "\n",
        "except Exception as e:\n",
        "  print(f'Error loading FastText model: {e}')"
      ],
      "metadata": {
        "id": "uksaWKy5OzOH"
      },
      "id": "uksaWKy5OzOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6e04f07b",
      "metadata": {
        "id": "6e04f07b"
      },
      "source": [
        "## Identify language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45271ac8",
      "metadata": {
        "id": "45271ac8"
      },
      "outputs": [],
      "source": [
        "texts = messages_df['basic_clean'].tolist()\n",
        "\n",
        "predictions = model.predict(texts)\n",
        "\n",
        "messages_df['lang_detected'] = [label[0].replace('__label__', '') for label in predictions[0]]\n",
        "messages_df['lang_confidence'] = [float(score[0]) for score in predictions[1]]\n",
        "\n",
        "lang_stats = messages_df.groupby('lang_detected')['lang_confidence'].agg(\n",
        "  count='count',\n",
        "  min='min',\n",
        "  q25=lambda x: x.quantile(0.25),\n",
        "  mean='mean',\n",
        "  median='median',\n",
        "  q90=lambda x: x.quantile(0.9),\n",
        "  max='max'\n",
        ").reset_index().sort_values(by='count', ascending=False)\n",
        "\n",
        "display(lang_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b29d9de",
      "metadata": {
        "id": "3b29d9de"
      },
      "source": [
        "## Filter text with Bahasa Indonesia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf620a7",
      "metadata": {
        "id": "9bf620a7"
      },
      "outputs": [],
      "source": [
        "messages_df = messages_df[\n",
        "  (messages_df['lang_detected'] == 'id')\n",
        "].copy()\n",
        "\n",
        "display(messages_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze data distribution"
      ],
      "metadata": {
        "id": "t6QTDBnfRj3o"
      },
      "id": "t6QTDBnfRj3o"
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_counts = messages_df.groupby(messages_df['date'].dt.to_period('M')).size()\n",
        "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
        "\n",
        "quarterly_counts = messages_df.groupby(messages_df['date'].dt.to_period('Q')).size()\n",
        "quarterly_counts.index = quarterly_counts.index.to_timestamp()\n",
        "\n",
        "def get_semester(date):\n",
        "  if pd.isnull(date):\n",
        "    return None\n",
        "  return f'{date.year}-S1' if date.month <= 6 else f'{date.year}-S2'\n",
        "\n",
        "messages_df['semester'] = messages_df['date'].apply(get_semester)\n",
        "semesterly_counts = messages_df.groupby('semester').size()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# monthly_counts.sort_index().plot(label='Monthly')\n",
        "quarterly_counts.sort_index().plot(label='Quarterly')\n",
        "# semesterly_counts.sort_index().plot(label='Semesterly')\n",
        "\n",
        "plt.title('Message Volume Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Messages')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gwNnMO7rRkQI"
      },
      "id": "gwNnMO7rRkQI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00eb2efe",
      "metadata": {
        "id": "00eb2efe"
      },
      "source": [
        "## Store language filtered dataframe to local file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ddc9e68",
      "metadata": {
        "id": "6ddc9e68"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "messages_df['date'] = messages_df['date'].dt.tz_localize(None)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_filename = f'language_filtered_telegram_messages_{timestamp}.xlsx'\n",
        "messages_df.to_excel(excel_filename, index=False)\n",
        "\n",
        "if env == 'colab':\n",
        "  file_path = f'/content/drive/MyDrive/telegram-data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "elif env == 'conda':\n",
        "  file_path = f'./data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "else:\n",
        "  print('Unable to detect suitable environment!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary library for pos tagging and stemming"
      ],
      "metadata": {
        "id": "jz9fOmW_WAbD"
      },
      "id": "jz9fOmW_WAbD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Core\n",
        "%pip install numpy==1.25.2\n",
        "%pip install pandas==1.5.3\n",
        "%pip install scipy==1.10.1\n",
        "%pip install scikit-learn==1.2.2\n",
        "%pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# NLP\n",
        "%pip install flair==0.12.2"
      ],
      "metadata": {
        "id": "Wqv22mEWWEB1"
      },
      "id": "Wqv22mEWWEB1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train POS tagging"
      ],
      "metadata": {
        "id": "IOPABdhCkE2N"
      },
      "id": "IOPABdhCkE2N"
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.datasets import UD_INDONESIAN\n",
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "corpus = UD_INDONESIAN()\n",
        "\n",
        "tag_type = 'upos'\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "embedding = WordEmbeddings('id')\n",
        "\n",
        "tagger = SequenceTagger(\n",
        "  hidden_size=256,\n",
        "  embeddings=embedding,\n",
        "  tag_dictionary=tag_dictionary,\n",
        "  tag_type=tag_type,\n",
        "  use_crf=True\n",
        ")\n",
        "\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train(\n",
        "  base_path='pos-id-model',\n",
        "  learning_rate=0.1,\n",
        "  mini_batch_size=32,\n",
        "  max_epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "R4ZavFpmklJr"
      },
      "id": "R4ZavFpmklJr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to detect environment"
      ],
      "metadata": {
        "id": "ZWZ1m5JFonXb"
      },
      "id": "ZWZ1m5JFonXb"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "def detect_environment():\n",
        "  try:\n",
        "    import google.colab\n",
        "    return 'colab'\n",
        "  except ImportError:\n",
        "    pass\n",
        "\n",
        "  if 'CONDA_PREFIX' in os.environ or os.path.exists(os.path.join(sys.prefix, 'conda-meta')):\n",
        "    return 'conda'\n",
        "\n",
        "  return 'local'\n",
        "\n",
        "env = detect_environment()"
      ],
      "metadata": {
        "id": "WU7JL-ECosUM"
      },
      "id": "WU7JL-ECosUM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store POS tagger model"
      ],
      "metadata": {
        "id": "RUBPEGk1rqhL"
      },
      "id": "RUBPEGk1rqhL"
    },
    {
      "cell_type": "code",
      "source": [
        "if env == 'colab':\n",
        "  dir_name = 'pos-id-model'\n",
        "  dir_source = f'/content/{dir_name}'\n",
        "  dir_target = f'/content/drive/MyDrive/models/{dir_name}'\n",
        "  !mkdir -p $(dirname $dir_target)\n",
        "  !cp -r $dir_source $dir_target\n",
        "  print(f'Successfully copied {dir_name} to {dir_target}')"
      ],
      "metadata": {
        "id": "K7RzXTI_rq_I"
      },
      "id": "K7RzXTI_rq_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "796c543a",
      "metadata": {
        "id": "796c543a"
      },
      "source": [
        "## Retrieve POS tagger model from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e07376",
      "metadata": {
        "id": "b9e07376"
      },
      "outputs": [],
      "source": [
        "if env == 'colab':\n",
        "  dir_name = 'pos-id-model'\n",
        "  dir_source = f'/content/drive/MyDrive/models/{dir_name}'\n",
        "  dir_target = f'/content/{dir_name}'\n",
        "  !mkdir -p $(dirname $dir_target)\n",
        "  !cp -r $dir_source $dir_target\n",
        "  print(f'Successfully copied {dir_name} to {dir_target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict POS tagging"
      ],
      "metadata": {
        "id": "vu9OVqQRMvsn"
      },
      "id": "vu9OVqQRMvsn"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0LBmQWrMwB6"
      },
      "id": "w0LBmQWrMwB6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter relevant POS and them stem"
      ],
      "metadata": {
        "id": "KKAaMR_lXQJy"
      },
      "id": "KKAaMR_lXQJy"
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "allowed_tags = {'NOUN', 'VERB', 'ADJ'}\n",
        "\n",
        "def filter_and_stem(tagged_tokens):\n",
        "  stemmed = []\n",
        "  for word, tag in tagged_tokens:\n",
        "    if tag in allowed_tags:\n",
        "      stemmed_word = stemmer.stem(word)\n",
        "      if stemmed_word and len(stemmed_word) > 2:\n",
        "        stemmed.append(stemmed_word)\n",
        "  return stemmed\n",
        "\n",
        "messages_df['stemmed_tokens'] = messages_df['pos_tags'].apply(filter_and_stem)\n",
        "messages_df = messages_df[messages_df['stemmed_tokens'].apply(lambda x: isinstance(x, list) and len(x) > 0)]"
      ],
      "metadata": {
        "id": "yEi6lnfmXSO4"
      },
      "id": "yEi6lnfmXSO4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze data distribution"
      ],
      "metadata": {
        "id": "uQy9R1W4RoIT"
      },
      "id": "uQy9R1W4RoIT"
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_counts = messages_df.groupby(messages_df['date'].dt.to_period('M')).size()\n",
        "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
        "\n",
        "quarterly_counts = messages_df.groupby(messages_df['date'].dt.to_period('Q')).size()\n",
        "quarterly_counts.index = quarterly_counts.index.to_timestamp()\n",
        "\n",
        "def get_semester(date):\n",
        "  if pd.isnull(date):\n",
        "    return None\n",
        "  return f'{date.year}-S1' if date.month <= 6 else f'{date.year}-S2'\n",
        "\n",
        "messages_df['semester'] = messages_df['date'].apply(get_semester)\n",
        "semesterly_counts = messages_df.groupby('semester').size()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# monthly_counts.sort_index().plot(label='Monthly')\n",
        "quarterly_counts.sort_index().plot(label='Quarterly')\n",
        "# semesterly_counts.sort_index().plot(label='Semesterly')\n",
        "\n",
        "plt.title('Message Volume Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Messages')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dvph1jwPRoaC"
      },
      "id": "Dvph1jwPRoaC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store POS tagged and filtered messsages"
      ],
      "metadata": {
        "id": "MN2bBFVWZmeM"
      },
      "id": "MN2bBFVWZmeM"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "messages_df['date'] = messages_df['date'].dt.tz_localize(None)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_filename = f'pos_tagged_telegram_messages_{timestamp}.xlsx'\n",
        "messages_df.to_excel(excel_filename, index=False)\n",
        "\n",
        "if env == 'colab':\n",
        "  file_path = f'/content/drive/MyDrive/telegram-data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "elif env == 'conda':\n",
        "  file_path = f'./data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "else:\n",
        "  print('Unable to detect suitable environment!')"
      ],
      "metadata": {
        "id": "fPdUU5M9ZmzT"
      },
      "id": "fPdUU5M9ZmzT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve POS tagged and filtered messages"
      ],
      "metadata": {
        "id": "JCkopDQXZn6q"
      },
      "id": "JCkopDQXZn6q"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = ''\n",
        "\n",
        "try:\n",
        "  if env == 'colab':\n",
        "    file_path = f'/content/drive/MyDrive/telegram-data/{filename}'\n",
        "    messages_df = pd.read_excel(file_path)\n",
        "    messages_df['date'] = pd.to_datetime(messages_df['date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
        "    print(f'Successfully read from {file_path} to dataframe')\n",
        "  elif env == 'conda':\n",
        "    file_path = f'./data/{filename}'\n",
        "    messages_df = pd.read_excel(file_path)\n",
        "    messages_df['date'] = pd.to_datetime(messages_df['date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
        "    print(f'Successfully read from {file_path} to dataframe')\n",
        "  else:\n",
        "    print('Unable to detect suitable environment!')\n",
        "except FileNotFoundError:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "except Exception as e:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(messages_df)"
      ],
      "metadata": {
        "id": "jobIYWNuZoMX"
      },
      "id": "jobIYWNuZoMX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary library for topic modeling"
      ],
      "metadata": {
        "id": "IJAy8uICY01D"
      },
      "id": "IJAy8uICY01D"
    },
    {
      "cell_type": "code",
      "source": [
        "# Network Analysis\n",
        "%pip install networkx\n",
        "\n",
        "# Ploting\n",
        "%pip install matplotlib\n",
        "%pip install seaborn"
      ],
      "metadata": {
        "id": "y_NG3masY3sV"
      },
      "id": "y_NG3masY3sV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4c3f0010",
      "metadata": {
        "id": "4c3f0010"
      },
      "source": [
        "## Run LDA topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79ee2fa",
      "metadata": {
        "id": "e79ee2fa"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "def get_semester(date):\n",
        "  if pd.isnull(date):\n",
        "    return None\n",
        "  return f'{date.year}-S1' if date.month <= 6 else f'{date.year}-S2'\n",
        "\n",
        "messages_df['semester'] = messages_df['date'].apply(get_semester)\n",
        "\n",
        "messages_df['assigned_topic'] = None\n",
        "messages_df['topic_keywords'] = None\n",
        "\n",
        "semesterly_topics = defaultdict(dict)\n",
        "lda_objects = defaultdict(dict)\n",
        "\n",
        "for semester, group in messages_df.groupby(['semester']):\n",
        "  tokenized_docs_series = group['advanced_clean'].apply(str.split)\n",
        "\n",
        "  indexed_docs = [(idx, doc) for idx, doc in zip(group.index, tokenized_docs_series) if len(doc) > 0]\n",
        "  if len(indexed_docs) < 5:\n",
        "    continue\n",
        "\n",
        "  indices, tokenized_docs = zip(*indexed_docs)\n",
        "\n",
        "  dictionary = corpora.Dictionary(tokenized_docs)\n",
        "  dictionary.filter_extremes(no_below=5, no_above=0.85)\n",
        "\n",
        "  corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "  filtered = [(idx, doc_bow, doc) for idx, doc_bow, doc in zip(indices, corpus, tokenized_docs) if len(doc_bow) > 0]\n",
        "  if not filtered:\n",
        "    continue\n",
        "\n",
        "  indices, corpus, tokenized_docs = zip(*filtered)\n",
        "\n",
        "  num_topics = 5\n",
        "\n",
        "  lda_model = models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        "  )\n",
        "\n",
        "  topic_id_to_keywords = {\n",
        "    topic_id: ', '.join([word for word, _ in lda_model.show_topic(topic_id, topn=5)])\n",
        "    for topic_id in range(num_topics)\n",
        "  }\n",
        "\n",
        "  dominant_topics = []\n",
        "  topic_keywords = []\n",
        "\n",
        "  for doc_bow in corpus:\n",
        "    topic_probs = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
        "    dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
        "    dominant_topics.append(dominant_topic)\n",
        "    topic_keywords.append(topic_id_to_keywords[dominant_topic])\n",
        "\n",
        "  messages_df.loc[list(indices), 'assigned_topic'] = dominant_topics\n",
        "  messages_df.loc[list(indices), 'topic_keywords'] = topic_keywords\n",
        "\n",
        "  topics = [\n",
        "    {\n",
        "      'topic_id': topic_id,\n",
        "      'top_words': topic_id_to_keywords[topic_id].split(', ')\n",
        "    }\n",
        "    for topic_id in range(num_topics)\n",
        "  ]\n",
        "\n",
        "  coherence_model = CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=tokenized_docs,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        "  )\n",
        "\n",
        "  semesterly_topics[semester] = {\n",
        "    'topics': topics,\n",
        "    'coherence': coherence_model.get_coherence()\n",
        "  }\n",
        "\n",
        "  lda_objects[semester] = {\n",
        "    'model': lda_model,\n",
        "    'corpus': corpus,\n",
        "    'dictionary': dictionary\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic heatmap"
      ],
      "metadata": {
        "id": "FPNAB0xdjuIF"
      },
      "id": "FPNAB0xdjuIF"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "keyword_semester_freq = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for semester, topic_info in semesterly_topics.items():\n",
        "  for topic in topic_info['topics']:\n",
        "    for word in topic['top_words']:\n",
        "      keyword_semester_freq[word][semester] += 1\n",
        "\n",
        "heatmap_df = pd.DataFrame(keyword_semester_freq).T.fillna(0).astype(int)\n",
        "\n",
        "top_keywords = heatmap_df.sum(axis=1).sort_values(ascending=False).head(20).index\n",
        "heatmap_df = heatmap_df.loc[top_keywords]\n",
        "heatmap_df = heatmap_df.sort_index(axis=1)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(heatmap_df, annot=True, fmt='d', cmap='YlGnBu')\n",
        "plt.title('Keyword Frequency Across Semesters')\n",
        "plt.xlabel('Semester')\n",
        "plt.ylabel('Keyword')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0x6nYIV9jueP"
      },
      "id": "0x6nYIV9jueP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify knowledge seeker, contributor, or neutral"
      ],
      "metadata": {
        "id": "GDeZjGBhA8_B"
      },
      "id": "GDeZjGBhA8_B"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from datetime import timedelta\n",
        "\n",
        "messages_df['assigned_topic'] = messages_df['assigned_topic'].astype('Int64')\n",
        "\n",
        "def get_semester(date):\n",
        "  if pd.isnull(date):\n",
        "    return None\n",
        "  return f'{date.year}-S1' if date.month <= 6 else f'{date.year}-S2'\n",
        "\n",
        "messages_df['semester'] = messages_df['date'].apply(get_semester)\n",
        "\n",
        "def get_semester_range(semester_str, buffer_days=30):\n",
        "  year, half = semester_str.split('-S')\n",
        "  year = int(year)\n",
        "\n",
        "  if half == '1':\n",
        "    start = pd.Timestamp(f'{year}-01-01')\n",
        "    end = pd.Timestamp(f'{year}-06-30')\n",
        "  else:\n",
        "    start = pd.Timestamp(f'{year}-07-01')\n",
        "    end = pd.Timestamp(f'{year}-12-31')\n",
        "\n",
        "  return start - timedelta(days=buffer_days), end\n",
        "\n",
        "role_dist_results = []\n",
        "top_users_results = []\n",
        "\n",
        "sem_topic_pairs = (\n",
        "  messages_df.dropna(subset=['assigned_topic'])[['semester', 'assigned_topic']]\n",
        "  .drop_duplicates()\n",
        "  .sort_values(['semester', 'assigned_topic'])\n",
        ")\n",
        "\n",
        "for _, row in sem_topic_pairs.iterrows():\n",
        "  semester = row['semester']\n",
        "  topic = row['assigned_topic']\n",
        "  if pd.isnull(semester) or pd.isnull(topic):\n",
        "    continue\n",
        "\n",
        "  topic_keywords = (\n",
        "    messages_df[\n",
        "      (messages_df['semester'] == semester) &\n",
        "      (messages_df['assigned_topic'] == topic)\n",
        "    ]['topic_keywords']\n",
        "    .dropna()\n",
        "    .unique()\n",
        "  )\n",
        "\n",
        "  topic_keywords = ', '.join(topic_keywords) if len(topic_keywords) > 0 else ''\n",
        "\n",
        "  buffered_start, semester_end = get_semester_range(semester, buffer_days=30)\n",
        "\n",
        "  subset = messages_df[\n",
        "    (messages_df['date'] >= buffered_start) &\n",
        "    (messages_df['date'] <= semester_end) &\n",
        "    (messages_df['assigned_topic'] == topic)\n",
        "  ].copy()\n",
        "\n",
        "  if subset.empty:\n",
        "    continue\n",
        "\n",
        "  id_to_sender = subset.set_index('id')['sender_id'].to_dict()\n",
        "\n",
        "  reply_msgs = subset[\n",
        "    (subset['date'] >= get_semester_range(semester)[0] + timedelta(days=30)) &\n",
        "    (subset['date'] <= semester_end) &\n",
        "    (subset['reply_to_msg_id'].notna())\n",
        "  ]\n",
        "\n",
        "  edges = []\n",
        "  for _, msg in reply_msgs.iterrows():\n",
        "    reply_to_id = msg['reply_to_msg_id']\n",
        "    replier = msg['sender_id']\n",
        "    original_sender = id_to_sender.get(reply_to_id)\n",
        "\n",
        "    if pd.notna(replier) and pd.notna(original_sender) and replier != original_sender:\n",
        "      edges.append((replier, original_sender))\n",
        "\n",
        "  G = nx.DiGraph()\n",
        "  G.add_edges_from(edges)\n",
        "\n",
        "  in_deg = dict(G.in_degree())\n",
        "  out_deg = dict(G.out_degree())\n",
        "  all_users = list(set(in_deg) | set(out_deg))\n",
        "\n",
        "  user_roles = pd.DataFrame({\n",
        "    'sender_id': all_users,\n",
        "    'in_degree': [in_deg.get(uid, 0) for uid in all_users],\n",
        "    'out_degree': [out_deg.get(uid, 0) for uid in all_users]\n",
        "  })\n",
        "\n",
        "  user_roles['role'] = user_roles.apply(\n",
        "    lambda row: 'contributor' if row['in_degree'] > row['out_degree']\n",
        "    else 'seeker' if row['out_degree'] > row['in_degree']\n",
        "    else 'neutral',\n",
        "    axis=1\n",
        "  )\n",
        "\n",
        "  semester_msgs = subset[\n",
        "    (subset['date'] >= get_semester_range(semester)[0] + timedelta(days=30)) &\n",
        "    (subset['date'] <= semester_end)\n",
        "  ]\n",
        "\n",
        "  msg_counts = semester_msgs.groupby('sender_id').size().reset_index(name='message_count')\n",
        "  user_roles = user_roles.merge(msg_counts, on='sender_id', how='left').fillna({'message_count': 0})\n",
        "\n",
        "  role_counts = user_roles['role'].value_counts().to_dict()\n",
        "\n",
        "  role_dist_results.append({\n",
        "    'semester': semester,\n",
        "    'assigned_topic': topic,\n",
        "    'topic_keywords': topic_keywords,\n",
        "    'contributor': role_counts.get('contributor', 0),\n",
        "    'seeker': role_counts.get('seeker', 0),\n",
        "    'neutral': role_counts.get('neutral', 0)\n",
        "  })\n",
        "\n",
        "  top_users = {}\n",
        "  for role in ['contributor', 'seeker', 'neutral']:\n",
        "    top = (\n",
        "      user_roles[user_roles['role'] == role]\n",
        "      .sort_values('message_count', ascending=False)\n",
        "      .head(3)[['sender_id', 'message_count']]\n",
        "      .to_dict('records')\n",
        "    )\n",
        "    top_users[f'top_{role}s'] = top\n",
        "\n",
        "  top_users_results.append({\n",
        "    'semester': semester,\n",
        "    'assigned_topic': topic,\n",
        "    'topic_keywords': topic_keywords,\n",
        "    **top_users\n",
        "  })\n",
        "\n",
        "role_distribution_topicwise = pd.DataFrame(role_dist_results)\n",
        "top_users_df = pd.DataFrame(top_users_results)"
      ],
      "metadata": {
        "id": "q5VeeN-nA9Up"
      },
      "id": "q5VeeN-nA9Up",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export"
      ],
      "metadata": {
        "id": "HJIFcpuvqW2v"
      },
      "id": "HJIFcpuvqW2v"
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_filename = f'topic_role_analysis_{timestamp}.xlsx'\n",
        "\n",
        "with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
        "  role_distribution_topicwise.to_excel(writer, sheet_name='Role Distribution', index=False)\n",
        "  top_users_df.to_excel(writer, sheet_name='Top Users', index=False)\n",
        "\n",
        "if env == 'colab':\n",
        "  file_path = f'/content/drive/MyDrive/telegram-data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "elif env == 'conda':\n",
        "  file_path = f'./data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "else:\n",
        "  print('Unable to detect suitable environment!')"
      ],
      "metadata": {
        "id": "Wl9r0M1KqXLk"
      },
      "id": "Wl9r0M1KqXLk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vu9OVqQRMvsn",
        "KKAaMR_lXQJy",
        "uQy9R1W4RoIT",
        "MN2bBFVWZmeM",
        "JCkopDQXZn6q",
        "IJAy8uICY01D",
        "4c3f0010",
        "FPNAB0xdjuIF",
        "GDeZjGBhA8_B",
        "HJIFcpuvqW2v"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}