{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalidjasir/ui-km-paper/blob/main/KM_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "pQi5SbcG0lkE"
      },
      "id": "pQi5SbcG0lkE"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Z4eJeJ0lNX",
        "outputId": "075050aa-ca35-47d3-94b3-9d0623090bfe"
      },
      "id": "b4Z4eJeJ0lNX",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c382e3",
      "metadata": {
        "id": "d9c382e3"
      },
      "source": [
        "## Install necessary library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0150f859-7fdf-4ec8-80d3-5246d0ba3d87",
      "metadata": {
        "id": "0150f859-7fdf-4ec8-80d3-5246d0ba3d87"
      },
      "outputs": [],
      "source": [
        "%pip install numpy==1.24.4\n",
        "%pip install scipy==1.10.1\n",
        "%pip install scikit-learn==1.2.2\n",
        "%pip install gensim==4.3.2\n",
        "%pip install pandas==1.5.3\n",
        "%pip install fasttext\n",
        "%pip install tqdm\n",
        "%pip install ipywidgets\n",
        "%pip install swifter\n",
        "%pip install emoji\n",
        "%pip install indonlp\n",
        "%pip install openpyxl\n",
        "%pip install telethon\n",
        "%pip install pytz\n",
        "%pip install python-dotenv\n",
        "%pip install networkx\n",
        "%pip install pyldavis\n",
        "%pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detect environment"
      ],
      "metadata": {
        "id": "Z_U3AOX9qmkY"
      },
      "id": "Z_U3AOX9qmkY"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def detect_environment():\n",
        "  try:\n",
        "    import google.colab\n",
        "    return 'colab'\n",
        "  except ImportError:\n",
        "    pass\n",
        "\n",
        "  if 'CONDA_PREFIX' in os.environ or os.path.exists(os.path.join(sys.prefix, 'conda-meta')):\n",
        "    return 'conda'\n",
        "\n",
        "  return 'local'\n",
        "\n",
        "env = detect_environment()"
      ],
      "metadata": {
        "id": "uzktgq0uqnDl"
      },
      "id": "uzktgq0uqnDl",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle Key"
      ],
      "metadata": {
        "id": "-6CQw26_vv1E"
      },
      "id": "-6CQw26_vv1E"
    },
    {
      "cell_type": "code",
      "source": [
        "api_id = ''\n",
        "api_hash = ''\n",
        "\n",
        "if env == 'colab':\n",
        "  from google.colab import userdata\n",
        "  api_id = userdata.get('api_id')\n",
        "  api_hash = userdata.get('api_hash')\n",
        "elif env == 'conda':\n",
        "  from dotenv import load_dotenv\n",
        "  load_dotenv()\n",
        "  api_id = os.getenv('api_id')\n",
        "  api_hash = os.getenv('api_hash')\n",
        "else:\n",
        "  print('Unable to detect suitable environment!')"
      ],
      "metadata": {
        "id": "XdJAZAwZvwIK"
      },
      "id": "XdJAZAwZvwIK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00581385",
      "metadata": {
        "id": "00581385"
      },
      "source": [
        "## Retrieve from telegram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74dfdc16-75fa-4582-a675-d87d16974d2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74dfdc16-75fa-4582-a675-d87d16974d2c",
        "outputId": "2df37fb2-d422-45c6-ee51-a2f2a2eccf6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written 10000 messages. Sleeping for 10 seconds...\n",
            "Written 20000 messages. Sleeping for 10 seconds...\n",
            "Written 30000 messages. Sleeping for 10 seconds...\n",
            "Written 40000 messages. Sleeping for 10 seconds...\n",
            "Written 50000 messages. Sleeping for 10 seconds...\n",
            "Written 60000 messages. Sleeping for 10 seconds...\n",
            "Written 70000 messages. Sleeping for 10 seconds...\n",
            "Written 80000 messages. Sleeping for 10 seconds...\n",
            "Written 90000 messages. Sleeping for 10 seconds...\n",
            "Written 100000 messages. Sleeping for 10 seconds...\n",
            "Written 110000 messages. Sleeping for 10 seconds...\n",
            "Written 120000 messages. Sleeping for 10 seconds...\n",
            "Written 130000 messages. Sleeping for 10 seconds...\n",
            "Written 140000 messages. Sleeping for 10 seconds...\n",
            "Written 150000 messages. Sleeping for 10 seconds...\n",
            "Written 160000 messages. Sleeping for 10 seconds...\n",
            "Written 170000 messages. Sleeping for 10 seconds...\n",
            "Written 180000 messages. Sleeping for 10 seconds...\n",
            "Written 190000 messages. Sleeping for 10 seconds...\n",
            "Written 200000 messages. Sleeping for 10 seconds...\n",
            "Written 210000 messages. Sleeping for 10 seconds...\n",
            "Written 220000 messages. Sleeping for 10 seconds...\n",
            "Written 230000 messages. Sleeping for 10 seconds...\n",
            "Written 240000 messages. Sleeping for 10 seconds...\n",
            "Written 250000 messages. Sleeping for 10 seconds...\n",
            "Written 260000 messages. Sleeping for 10 seconds...\n",
            "Written 270000 messages. Sleeping for 10 seconds...\n",
            "Written 280000 messages. Sleeping for 10 seconds...\n",
            "Written 290000 messages. Sleeping for 10 seconds...\n",
            "Written 300000 messages. Sleeping for 10 seconds...\n",
            "Written 310000 messages. Sleeping for 10 seconds...\n",
            "Written 320000 messages. Sleeping for 10 seconds...\n",
            "Written 330000 messages. Sleeping for 10 seconds...\n",
            "Final batch written. Total messages: 331206\n",
            "Done! Messages saved to telegram_messages_20250519_042802.csv.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "from telethon import TelegramClient\n",
        "from pytz import timezone\n",
        "\n",
        "session_name = 'my_session'\n",
        "channel_input = 'https://t.me/diskusipajak'\n",
        "wib_timezone = timezone('Asia/Jakarta')\n",
        "start_date = wib_timezone.localize(datetime(2015, 1, 1))\n",
        "end_date = wib_timezone.localize(datetime(2025, 4, 30))\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "csv_filename = f'telegram_messages_{timestamp}.csv'\n",
        "batch_size = 10000\n",
        "\n",
        "async def main():\n",
        "  async with TelegramClient(session_name, api_id, api_hash) as client:\n",
        "    channel = await client.get_entity(channel_input)\n",
        "\n",
        "    buffer = []\n",
        "    total_count = 0\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "      writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "      writer.writerow([\n",
        "        'id',\n",
        "        'date',\n",
        "        'text',\n",
        "        'sender_id',\n",
        "        'chat_id',\n",
        "        'reply_to_msg_id',\n",
        "        'views',\n",
        "        'forwards',\n",
        "        'buttons',\n",
        "        'raw_text',\n",
        "        'message_link'\n",
        "      ])\n",
        "\n",
        "    async for msg in client.iter_messages(channel, offset_date=start_date, reverse=True):\n",
        "      if msg.date > end_date:\n",
        "        break\n",
        "\n",
        "      row = [\n",
        "        msg.id,\n",
        "        msg.date.astimezone(wib_timezone).strftime('%a %b %d %H:%M:%S %z %Y') if msg.date else \"\",\n",
        "        msg.text.replace('\\n', ' ').strip() if msg.text else \"\",\n",
        "        msg.sender_id if msg.sender_id else '',\n",
        "        getattr(msg, 'chat_id', getattr(msg.to_id, 'channel_id', '')),\n",
        "        msg.reply_to_msg_id if msg.reply_to_msg_id else '',\n",
        "        msg.views if msg.views is not None else '',\n",
        "        msg.forwards if msg.forwards is not None else '',\n",
        "        len(msg.buttons) if msg.buttons else 0,\n",
        "        msg.raw_text.replace('\\n', ' ').strip() if msg.raw_text else '',\n",
        "        f'https://t.me/{channel.username}/{msg.id}'\n",
        "      ]\n",
        "\n",
        "      buffer.append(row)\n",
        "      total_count += 1\n",
        "\n",
        "      if total_count % batch_size == 0:\n",
        "        with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
        "          writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "          writer.writerows(buffer)\n",
        "\n",
        "        buffer.clear()\n",
        "        print(f'Written {total_count} messages. Sleeping for 10 seconds...')\n",
        "        time.sleep(10)\n",
        "\n",
        "    if buffer:\n",
        "      with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "        writer.writerows(buffer)\n",
        "      print(f'Final batch written. Total messages: {total_count}')\n",
        "\n",
        "    print(f'Done! Messages saved to {csv_filename}.')\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move file for later processing"
      ],
      "metadata": {
        "id": "x3Mlc4ch0t3R"
      },
      "id": "x3Mlc4ch0t3R"
    },
    {
      "cell_type": "code",
      "source": [
        "if env == 'colab':\n",
        "  file_path = '/content/drive/MyDrive/telegram-data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $csv_filename $file_path\n",
        "  print(f'Successfully moved {csv_filename} to {file_path}')\n",
        "elif env == 'conda':\n",
        "  file_path = './data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $csv_filename $file_path\n",
        "  print(f'Successfully moved {csv_filename} to {file_path}')\n",
        "else:\n",
        "  print('Unable to detect suitable environment! No file moved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BerT_yDW0tYc",
        "outputId": "14d40b28-4030-4e43-efda-8f3dbd94e7dc"
      },
      "id": "BerT_yDW0tYc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully moved telegram_messages_20250519_042802.csv to /content/drive/MyDrive/telegram-data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3615095",
      "metadata": {
        "id": "b3615095"
      },
      "source": [
        "## Retrieve stored telegram messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e5e7a4",
      "metadata": {
        "id": "c0e5e7a4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = 'telegram_messages_20250519_042802.csv'\n",
        "\n",
        "dtype = {\n",
        "  'id': 'string',\n",
        "  'date': 'string',\n",
        "  'text': 'string',\n",
        "  'sender_id': 'string',\n",
        "  'chat_id': 'string',\n",
        "  'reply_to_msg_id': 'string',\n",
        "  'views': 'Int64',\n",
        "  'forwards': 'Int64',\n",
        "  'buttons': 'Int64',\n",
        "  'raw_text': 'string',\n",
        "  'message_link': 'string'\n",
        "}\n",
        "\n",
        "try:\n",
        "  if env == 'colab':\n",
        "    file_path = f'/content/drive/MyDrive/telegram-data/{filename}'\n",
        "    messages_df = pd.read_csv(file_path, dtype=dtype)\n",
        "    messages_df['date'] = pd.to_datetime(messages_df['date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
        "    print(f'Successfully read from {file_path} to dataframe')\n",
        "  elif env == 'conda':\n",
        "    file_path = f'./data/{filename}'\n",
        "    messages_df = pd.read_csv(file_path, dtype=dtype)\n",
        "    messages_df['date'] = pd.to_datetime(messages_df['date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
        "    print(f'Successfully read from {file_path} to dataframe')\n",
        "  else:\n",
        "    print('Unable to detect suitable environment! Nothing loaded to dataframe')\n",
        "except FileNotFoundError:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "except Exception as e:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(messages_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify knowledge seeker and contributor"
      ],
      "metadata": {
        "id": "GDeZjGBhA8_B"
      },
      "id": "GDeZjGBhA8_B"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "network_df = messages_df.dropna(subset=['reply_to_msg_id'])\n",
        "\n",
        "id_to_sender = messages_df.set_index('id')['sender_id'].to_dict()\n",
        "\n",
        "edges = []\n",
        "for _, row in network_df.iterrows():\n",
        "  original_sender = id_to_sender.get(row['reply_to_msg_id'])\n",
        "  if pd.notna(original_sender) and pd.notna(row['sender_id']) and original_sender != row['sender_id']:\n",
        "    edges.append((row['sender_id'], original_sender))\n",
        "\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "in_deg = dict(G.in_degree())\n",
        "out_deg = dict(G.out_degree())\n",
        "\n",
        "user_roles = pd.DataFrame({\n",
        "  'in_degree': pd.Series(in_deg),\n",
        "  'out_degree': pd.Series(out_deg)\n",
        "}).fillna(0)\n",
        "\n",
        "user_roles.index.name = 'sender_id'\n",
        "user_roles = user_roles.reset_index()\n",
        "\n",
        "user_roles['role'] = user_roles.apply(\n",
        "  lambda row: 'contributor' if row['in_degree'] >= row['out_degree']\n",
        "  else 'seeker', axis=1\n",
        ")\n",
        "\n",
        "messages_df['year'] = messages_df['date'].dt.year\n",
        "messages_df['month'] = messages_df['date'].dt.month\n",
        "\n",
        "messages_df = messages_df.drop(columns=['in_degree', 'out_degree', 'role'], errors='ignore')\n",
        "messages_df = messages_df.merge(user_roles, on='sender_id', how='left')\n",
        "\n",
        "display(messages_df)\n"
      ],
      "metadata": {
        "id": "q5VeeN-nA9Up"
      },
      "id": "q5VeeN-nA9Up",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging reply with original message"
      ],
      "metadata": {
        "id": "zpFyg1DpNSL2"
      },
      "id": "zpFyg1DpNSL2"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "messages_df.dropna(subset=[\n",
        "  'text',\n",
        "  'raw_text',\n",
        "  'in_degree',\n",
        "  'out_degree',\n",
        "  'role'\n",
        "], inplace=True)\n",
        "\n",
        "row_lookup = {\n",
        "  (row['chat_id'], row['id']): row.to_dict() for _, row in messages_df.iterrows()\n",
        "}\n",
        "\n",
        "def collect_reply_chain(chat_id, msg_id, visited=None):\n",
        "  if visited is None:\n",
        "    visited = set()\n",
        "\n",
        "  key = (chat_id, msg_id)\n",
        "  if key in visited:\n",
        "    return []\n",
        "  visited.add(key)\n",
        "\n",
        "  current_row = row_lookup.get(key)\n",
        "  if current_row is None:\n",
        "    return []\n",
        "\n",
        "  reply_to_id = current_row.get('reply_to_msg_id')\n",
        "  if pd.isna(reply_to_id):\n",
        "    return [current_row]\n",
        "\n",
        "  parent_chain = collect_reply_chain(chat_id, reply_to_id, visited)\n",
        "  return parent_chain + [current_row]\n",
        "\n",
        "def format_chain_sorted(chain):\n",
        "  chain_sorted = sorted(chain, key=lambda x: x['date'])\n",
        "  return ' RT '.join(str(row.get('text') or '') for row in chain_sorted)\n",
        "\n",
        "messages_df['merged_text'] = messages_df.apply(\n",
        "  lambda row: format_chain_sorted(collect_reply_chain(row['chat_id'], row['id'])), axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "hrF-zsqMNShS"
      },
      "id": "hrF-zsqMNShS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "60e93023",
      "metadata": {
        "id": "60e93023"
      },
      "source": [
        "## Pre-processing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7252cbd4-5782-4005-9397-620fbe6b11d3",
      "metadata": {
        "id": "7252cbd4-5782-4005-9397-620fbe6b11d3"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import emoji\n",
        "import re\n",
        "import string\n",
        "import indoNLP.preprocessing\n",
        "\n",
        "def basic_clean(text):\n",
        "  # Normalize Unicode (remove fancy fonts, underlines)\n",
        "  text = unicodedata.normalize('NFKD', text)\n",
        "  text = ''.join(c for c in text if not unicodedata.combining(c))\n",
        "\n",
        "  # Translate emojis\n",
        "  text = emoji.demojize(text, delimiters=(' ', ' '), language='id')\n",
        "\n",
        "  # Remove HTML\n",
        "  text = indoNLP.preprocessing.remove_html(text)\n",
        "\n",
        "  # Remove URL\n",
        "  text = indoNLP.preprocessing.remove_url(text)\n",
        "\n",
        "  # Remove usernames\n",
        "  text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "  # Remove RT\n",
        "  text = re.sub(r'\\brt\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "  # Remove hashtag symbol but keep the word\n",
        "  text = re.sub(r'#', '', text)\n",
        "\n",
        "  # Remove extra whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "def advanced_clean(text):\n",
        "  # Remove word elongation\n",
        "  text = indoNLP.preprocessing.replace_word_elongation(text)\n",
        "\n",
        "  # Replace slang\n",
        "  text = indoNLP.preprocessing.replace_slang(text)\n",
        "\n",
        "  # Lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Remove digits\n",
        "  text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "  # Remove stopwords\n",
        "  text = indoNLP.preprocessing.remove_stopwords(text)\n",
        "\n",
        "  # Remove extra whitespace again (just in case)\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9cc6cb3",
      "metadata": {
        "id": "a9cc6cb3"
      },
      "source": [
        "## Run pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1901ff96-5c36-4386-a83a-ecfb01bdc8f5",
      "metadata": {
        "id": "1901ff96-5c36-4386-a83a-ecfb01bdc8f5"
      },
      "outputs": [],
      "source": [
        "import swifter\n",
        "\n",
        "messages_df.dropna(subset=[\n",
        "  'text',\n",
        "  'raw_text',\n",
        "  'in_degree',\n",
        "  'out_degree',\n",
        "  'role',\n",
        "  'merged_text'\n",
        "], inplace=True)\n",
        "messages_df['basic_clean'] = messages_df['merged_text'].swifter.apply(basic_clean)\n",
        "messages_df['advanced_clean'] = messages_df['basic_clean'].swifter.apply(advanced_clean)\n",
        "messages_df.dropna(subset=['basic_clean', 'advanced_clean'], inplace=True)\n",
        "\n",
        "display(messages_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8f8011",
      "metadata": {
        "id": "3e8f8011"
      },
      "source": [
        "## Filter out text that is shorter than 5 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478d01ee",
      "metadata": {
        "id": "478d01ee"
      },
      "outputs": [],
      "source": [
        "telegram_df = messages_df[messages_df['advanced_clean'].str.split().str.len() > 5].copy()\n",
        "\n",
        "display(telegram_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving language model"
      ],
      "metadata": {
        "id": "JP9ILVEjOy0F"
      },
      "id": "JP9ILVEjOy0F"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fasttext\n",
        "import urllib.request\n",
        "\n",
        "filename = 'lid.176.bin'\n",
        "model_url = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin'\n",
        "model_path = ''\n",
        "\n",
        "try:\n",
        "  if env == 'colab':\n",
        "    model_dir = '/content/drive/MyDrive/models'\n",
        "  elif env == 'conda':\n",
        "    model_dir = './models'\n",
        "  else:\n",
        "    raise EnvironmentError('Unable to detect suitable environment!')\n",
        "\n",
        "  os.makedirs(model_dir, exist_ok=True)\n",
        "  model_path = os.path.join(model_dir, filename)\n",
        "\n",
        "  if not os.path.exists(model_path):\n",
        "    print('Model not found locally. Downloading...')\n",
        "    urllib.request.urlretrieve(model_url, model_path)\n",
        "    print('Download completed!')\n",
        "\n",
        "  print(f'Loading model from {model_path}...')\n",
        "  model = fasttext.load_model(model_path)\n",
        "  print('Model loaded successfully!')\n",
        "\n",
        "except Exception as e:\n",
        "  print(f'Error loading FastText model: {e}')"
      ],
      "metadata": {
        "id": "uksaWKy5OzOH"
      },
      "id": "uksaWKy5OzOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6e04f07b",
      "metadata": {
        "id": "6e04f07b"
      },
      "source": [
        "## Identify language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45271ac8",
      "metadata": {
        "id": "45271ac8"
      },
      "outputs": [],
      "source": [
        "texts = telegram_df['advanced_clean'].tolist()\n",
        "\n",
        "predictions = model.predict(texts)\n",
        "\n",
        "telegram_df['lang_detected'] = [label[0].replace('__label__', '') for label in predictions[0]]\n",
        "telegram_df['lang_confidence'] = [float(score[0]) for score in predictions[1]]\n",
        "\n",
        "lang_stats = telegram_df.groupby('lang_detected')['lang_confidence'].agg(\n",
        "  count='count',\n",
        "  min='min',\n",
        "  q25=lambda x: x.quantile(0.25),\n",
        "  mean='mean',\n",
        "  median='median',\n",
        "  q90=lambda x: x.quantile(0.9),\n",
        "  max='max'\n",
        ").reset_index().sort_values(by='count', ascending=False)\n",
        "\n",
        "display(lang_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b29d9de",
      "metadata": {
        "id": "3b29d9de"
      },
      "source": [
        "## Filter text with Bahasa Indonesia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf620a7",
      "metadata": {
        "id": "9bf620a7"
      },
      "outputs": [],
      "source": [
        "filtered_df = telegram_df[\n",
        "  (telegram_df['lang_detected'] == 'id')\n",
        "].copy()\n",
        "\n",
        "display(filtered_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7fca565",
      "metadata": {
        "id": "b7fca565"
      },
      "source": [
        "## Find similarity with KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f65a8d6f",
      "metadata": {
        "id": "f65a8d6f"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_df['advanced_clean'])\n",
        "\n",
        "nn = NearestNeighbors(metric='cosine', n_neighbors=10)\n",
        "nn.fit(tfidf_matrix)\n",
        "\n",
        "distances, indices = nn.kneighbors(tfidf_matrix)\n",
        "\n",
        "threshold = 0.95\n",
        "to_remove = set()\n",
        "\n",
        "for i, (dists, neighbors) in enumerate(zip(distances, indices)):\n",
        "  for dist, idx in zip(dists[1:], neighbors[1:]):\n",
        "    sim = 1 - dist\n",
        "    if sim >= threshold:\n",
        "      to_remove.add(max(i, idx))\n",
        "\n",
        "unique_df = filtered_df.drop(filtered_df.index[list(to_remove)]).reset_index(drop=True)\n",
        "\n",
        "display(unique_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00eb2efe",
      "metadata": {
        "id": "00eb2efe"
      },
      "source": [
        "## Store to local file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ddc9e68",
      "metadata": {
        "id": "6ddc9e68"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "unique_df['date'] = unique_df['date'].dt.tz_localize(None)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_filename = f'filtered_telegram_messages_{timestamp}.xlsx'\n",
        "unique_df.to_excel(excel_filename, index=False)\n",
        "\n",
        "if env == 'colab':\n",
        "  file_path = f'/content/drive/MyDrive/telegram-data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "elif env == 'conda':\n",
        "  file_path = f'./data/'\n",
        "  !mkdir -p $file_path\n",
        "  !mv $excel_filename $file_path\n",
        "  print(f'Successfully moved {excel_filename} to {file_path}')\n",
        "else:\n",
        "  print('Unable to detect suitable environment!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796c543a",
      "metadata": {
        "id": "796c543a"
      },
      "source": [
        "## Retrieve filtered messeges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e07376",
      "metadata": {
        "id": "b9e07376"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = 'filtered_telegram_messages_20250521_040024.xlsx'\n",
        "\n",
        "try:\n",
        "  if env == 'colab':\n",
        "    file_path = f'/content/drive/MyDrive/telegram-data/{filename}'\n",
        "    unique_df = pd.read_excel(file_path)\n",
        "    unique_df['date'] = pd.to_datetime(unique_df['date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
        "    print(f'Successfully read from {file_path} to dataframe')\n",
        "  elif env == 'conda':\n",
        "    file_path = f'./data/{filename}'\n",
        "    unique_df = pd.read_excel(file_path)\n",
        "    unique_df['date'] = pd.to_datetime(unique_df['date'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')\n",
        "    print(f'Successfully read from {file_path} to dataframe')\n",
        "  else:\n",
        "    print('Unable to detect suitable environment!')\n",
        "except FileNotFoundError:\n",
        "  unique_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "except Exception as e:\n",
        "  unique_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(unique_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3f0010",
      "metadata": {
        "id": "4c3f0010"
      },
      "source": [
        "## Run LDA topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e79ee2fa",
      "metadata": {
        "id": "e79ee2fa"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "import pandas as pd\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import os\n",
        "import json\n",
        "\n",
        "os.makedirs('lda_jsons', exist_ok=True)\n",
        "\n",
        "monthly_yearly_topics = defaultdict(dict)\n",
        "lda_objects = defaultdict(dict)\n",
        "metadata_dict = {}\n",
        "\n",
        "unique_df['assigned_topic'] = None\n",
        "unique_df['topic_keywords'] = None\n",
        "\n",
        "for (month, year), group in unique_df.groupby(['month', 'year']):\n",
        "  tokenized_docs_series = group['advanced_clean'].apply(str.split)\n",
        "\n",
        "  indexed_docs = list(zip(group.index, tokenized_docs_series))\n",
        "  indexed_docs = [(idx, doc) for idx, doc in indexed_docs if len(doc) > 0]\n",
        "\n",
        "  if len(indexed_docs) < 5:\n",
        "    continue\n",
        "\n",
        "  indices, tokenized_docs = zip(*indexed_docs)\n",
        "\n",
        "  dictionary = corpora.Dictionary(tokenized_docs)\n",
        "  dictionary.filter_extremes(no_below=5, no_above=0.85)\n",
        "\n",
        "  corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "  filtered = [(idx, doc_bow, doc) for idx, doc_bow, doc in zip(indices, corpus, tokenized_docs) if len(doc_bow) > 0]\n",
        "  if not filtered:\n",
        "    continue\n",
        "\n",
        "  indices, corpus, tokenized_docs = zip(*filtered)\n",
        "\n",
        "  num_topics = 5\n",
        "\n",
        "  lda_model = models.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=10,\n",
        "    alpha='auto',\n",
        "    per_word_topics=True\n",
        "  )\n",
        "\n",
        "  topic_id_to_keywords = {\n",
        "    topic_id: ', '.join([word for word, _ in lda_model.show_topic(topic_id, topn=5)])\n",
        "    for topic_id in range(num_topics)\n",
        "  }\n",
        "\n",
        "  dominant_topics = []\n",
        "  topic_keywords = []\n",
        "\n",
        "  for doc_bow in corpus:\n",
        "    topic_probs = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
        "    dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
        "    dominant_topics.append(dominant_topic)\n",
        "    topic_keywords.append(topic_id_to_keywords[dominant_topic])\n",
        "\n",
        "  unique_df.loc[list(indices), 'assigned_topic'] = dominant_topics\n",
        "  unique_df.loc[list(indices), 'topic_keywords'] = topic_keywords\n",
        "\n",
        "  topics = [\n",
        "    {\n",
        "      'topic_id': topic_id,\n",
        "      'top_words': topic_id_to_keywords[topic_id].split(', ')\n",
        "    }\n",
        "    for topic_id in range(num_topics)\n",
        "  ]\n",
        "\n",
        "  coherence_model = CoherenceModel(\n",
        "    model=lda_model,\n",
        "    texts=tokenized_docs,\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        "  )\n",
        "\n",
        "  monthly_yearly_topics[month][year] = {\n",
        "    'topics': topics,\n",
        "    'coherence': coherence_model.get_coherence()\n",
        "  }\n",
        "\n",
        "  lda_objects[month][year] = {\n",
        "    'model': lda_model,\n",
        "    'corpus': corpus,\n",
        "    'dictionary': dictionary\n",
        "  }\n",
        "\n",
        "  group_with_topics = unique_df.loc[list(indices)]\n",
        "\n",
        "  role_counts = group_with_topics.groupby(['sender_id', 'role']).size().reset_index(name='count')\n",
        "\n",
        "  top_count = 3\n",
        "  top_contributor = role_counts[role_counts['role'] == 'contributor'].sort_values('count', ascending=False).head(top_count)\n",
        "  top_seeker = role_counts[role_counts['role'] == 'seeker'].sort_values('count', ascending=False).head(top_count)\n",
        "\n",
        "  top_contributor_info = top_contributor[['sender_id', 'count']].to_dict('records')\n",
        "  top_seeker_info = top_seeker[['sender_id', 'count']].to_dict('records')\n",
        "\n",
        "  monthly_yearly_topics[month][year]['top_contributor'] = top_contributor_info\n",
        "  monthly_yearly_topics[month][year]['top_seeker'] = top_seeker_info\n",
        "\n",
        "  metadata_dict[f'{month}_{year}'] = {\n",
        "    'top_contributor': top_contributor_info,\n",
        "    'top_seeker': top_seeker_info\n",
        "  }\n",
        "\n",
        "  vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "  vis_json = vis_data.to_json()\n",
        "\n",
        "  json_filename = f'lda_jsons/lda_{month}_{year}.json'\n",
        "  with open(json_filename, 'w') as f:\n",
        "    f.write(vis_json)\n",
        "\n",
        "with open('lda_jsons/metadata.json', 'w') as f:\n",
        "  json.dump(metadata_dict, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move LDA JSON"
      ],
      "metadata": {
        "id": "cuHf83oB00yq"
      },
      "id": "cuHf83oB00yq"
    },
    {
      "cell_type": "code",
      "source": [
        "current_dir = './lda_jsons'\n",
        "\n",
        "if env == 'colab':\n",
        "  target_dir = f'/content/drive/MyDrive/interactive-lda/'\n",
        "  !mkdir -p $target_dir\n",
        "  !mv $current_dir $target_dir\n",
        "  print(f'Successfully moved {current_dir} to {target_dir}')\n",
        "elif env == 'conda':\n",
        "  target_dir = f'./interactive-lda/'\n",
        "  !mkdir -p $target_dir\n",
        "  !mv $current_dir $target_dir\n",
        "  print(f'Successfully moved {current_dir} to {target_dir}')\n",
        "else:\n",
        "  print('Unable to detect suitable environment!')"
      ],
      "metadata": {
        "id": "mSfNJDEl01NE"
      },
      "id": "mSfNJDEl01NE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}