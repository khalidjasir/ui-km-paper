{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "-6CQw26_vv1E",
      "metadata": {
        "id": "-6CQw26_vv1E"
      },
      "source": [
        "## Handle Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "XdJAZAwZvwIK",
      "metadata": {
        "id": "XdJAZAwZvwIK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "api_id = os.getenv('api_id')\n",
        "api_hash = os.getenv('api_hash')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00581385",
      "metadata": {
        "id": "00581385"
      },
      "source": [
        "## Retrieve from telegram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74dfdc16-75fa-4582-a675-d87d16974d2c",
      "metadata": {
        "id": "74dfdc16-75fa-4582-a675-d87d16974d2c"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "from datetime import datetime\n",
        "from telethon import TelegramClient\n",
        "from pytz import timezone\n",
        "\n",
        "session_name = 'my_session'\n",
        "channel_input = 'https://t.me/Bibitid'\n",
        "wib_timezone = timezone('Asia/Jakarta')\n",
        "start_date = wib_timezone.localize(datetime(2025, 5, 25))\n",
        "end_date = wib_timezone.localize(datetime(2025, 5, 27))\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "csv_filename = f'telegram_messages_{timestamp}.csv'\n",
        "batch_size = 10000\n",
        "\n",
        "async def main():\n",
        "  async with TelegramClient(session_name, api_id, api_hash) as client:\n",
        "    channel = await client.get_entity(channel_input)\n",
        "\n",
        "    buffer = []\n",
        "    total_count = 0\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "      writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "      writer.writerow([\n",
        "        'id',\n",
        "        'date',\n",
        "        'text',\n",
        "        'sender_id',\n",
        "        'chat_id',\n",
        "        'reply_to_msg_id',\n",
        "        'views',\n",
        "        'forwards',\n",
        "        'buttons',\n",
        "        'raw_text',\n",
        "        'message_link'\n",
        "      ])\n",
        "\n",
        "    async for msg in client.iter_messages(channel, offset_date=start_date, reverse=True):\n",
        "      if msg.date > end_date:\n",
        "        break\n",
        "\n",
        "      row = [\n",
        "        msg.id,\n",
        "        msg.date.astimezone(wib_timezone).strftime('%a %b %d %H:%M:%S %z %Y') if msg.date else \"\",\n",
        "        msg.text.replace('\\n', ' ').strip() if msg.text else \"\",\n",
        "        msg.sender_id if msg.sender_id else '',\n",
        "        getattr(msg, 'chat_id', getattr(msg.to_id, 'channel_id', '')),\n",
        "        msg.reply_to_msg_id if msg.reply_to_msg_id else '',\n",
        "        msg.views if msg.views is not None else '',\n",
        "        msg.forwards if msg.forwards is not None else '',\n",
        "        len(msg.buttons) if msg.buttons else 0,\n",
        "        msg.raw_text.replace('\\n', ' ').strip() if msg.raw_text else '',\n",
        "        f'https://t.me/{channel.username}/{msg.id}'\n",
        "      ]\n",
        "\n",
        "      buffer.append(row)\n",
        "      total_count += 1\n",
        "\n",
        "      if total_count % batch_size == 0:\n",
        "        with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
        "          writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "          writer.writerows(buffer)\n",
        "\n",
        "        buffer.clear()\n",
        "        print(f'Written {total_count} messages. Sleeping for 10 seconds...')\n",
        "        time.sleep(10)\n",
        "\n",
        "    if buffer:\n",
        "      with open(csv_filename, mode='a', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "        writer.writerows(buffer)\n",
        "      print(f'Final batch written. Total messages: {total_count}')\n",
        "\n",
        "    print(f'Done! Messages saved to {csv_filename}.')\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x3Mlc4ch0t3R",
      "metadata": {
        "id": "x3Mlc4ch0t3R"
      },
      "source": [
        "## Move file for later processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BerT_yDW0tYc",
      "metadata": {
        "id": "BerT_yDW0tYc"
      },
      "outputs": [],
      "source": [
        "file_path = './data/'\n",
        "!mkdir -p $file_path\n",
        "!mv $csv_filename $file_path\n",
        "print(f'Successfully moved {csv_filename} to {file_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3615095",
      "metadata": {
        "id": "b3615095"
      },
      "source": [
        "## Retrieve stored telegram messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e5e7a4",
      "metadata": {
        "id": "c0e5e7a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize()\n",
        "\n",
        "dtype = {\n",
        "  'id': 'string',\n",
        "  'date': 'string',\n",
        "  'text': 'string',\n",
        "  'sender_id': 'string',\n",
        "  'chat_id': 'string',\n",
        "  'reply_to_msg_id': 'string',\n",
        "  'views': 'Int64',\n",
        "  'forwards': 'Int64',\n",
        "  'buttons': 'Int64',\n",
        "  'raw_text': 'string',\n",
        "  'message_link': 'string'\n",
        "}\n",
        "\n",
        "try:\n",
        "  filename = 'telegram_messages_20250527_151905.csv'\n",
        "  file_path = f'./data/{filename}'\n",
        "\n",
        "  if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f'File {file_path} does not exist')\n",
        "  \n",
        "  print(f'Reading from {file_path} to dataframe')\n",
        "\n",
        "  original_df = pd.read_csv(file_path, dtype=dtype)\n",
        "  original_df['date'] = pd.to_datetime(\n",
        "    original_df['date'],\n",
        "    format='%a %b %d %H:%M:%S %z %Y',\n",
        "    errors='coerce'\n",
        "  )\n",
        "  original_df['date'] = original_df['date'].parallel_apply(\n",
        "    lambda x: x.replace(tzinfo=None) if pd.notnull(x) else x\n",
        "  )\n",
        "  print(f'Successfully read from {file_path} to dataframe')\n",
        "\n",
        "except FileNotFoundError:\n",
        "  original_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "\n",
        "except Exception as e:\n",
        "  original_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(original_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yEoTrZNWPlmt",
      "metadata": {
        "id": "yEoTrZNWPlmt"
      },
      "source": [
        "## Analyze data distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UJpusGEXPn_l",
      "metadata": {
        "id": "UJpusGEXPn_l"
      },
      "outputs": [],
      "source": [
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize()\n",
        "\n",
        "monthly_counts = original_df.groupby(original_df['date'].dt.to_period('M')).size()\n",
        "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
        "\n",
        "quarterly_counts = original_df.groupby(original_df['date'].dt.to_period('Q')).size()\n",
        "quarterly_counts.index = quarterly_counts.index.to_timestamp()\n",
        "\n",
        "def get_semester(date):\n",
        "  if pd.isnull(date):\n",
        "    return None\n",
        "  return f'{date.year}-S1' if date.month <= 6 else f'{date.year}-S2'\n",
        "\n",
        "original_df['semester'] = original_df['date'].parallel_apply(get_semester)\n",
        "semesterly_counts = original_df.groupby('semester').size()\n",
        "\n",
        "yearly_counts = original_df.groupby(original_df['date'].dt.to_period('Y')).size()\n",
        "yearly_counts.index = yearly_counts.index.to_timestamp()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# monthly_counts.sort_index().plot(label='Monthly')\n",
        "quarterly_counts.sort_index().plot(label='Quarterly')\n",
        "# semesterly_counts.sort_index().plot(label='Semesterly')\n",
        "# yearly_counts.sort_index().plot(label='Yearly')\n",
        "\n",
        "plt.title('Message Volume Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Messages')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee53727a",
      "metadata": {},
      "source": [
        "## Construct threaded messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8862484a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_merged_text(df):\n",
        "  msg_dict = df.set_index('id').to_dict('index')\n",
        "  merged_cache = {}\n",
        "\n",
        "  def get_merged_text(msg_id):\n",
        "    if msg_id in merged_cache:\n",
        "      return merged_cache[msg_id]\n",
        "\n",
        "    msg = msg_dict.get(msg_id)\n",
        "    if not msg:\n",
        "      return ''\n",
        "\n",
        "    current_text = msg.get('text') or ''\n",
        "\n",
        "    reply_id = msg.get('reply_to_msg_id')\n",
        "    if pd.notna(reply_id):\n",
        "      parent_text = get_merged_text(reply_id)\n",
        "      merged = f'{parent_text} RT {current_text}'\n",
        "    else:\n",
        "      merged = current_text\n",
        "\n",
        "    merged_cache[msg_id] = merged\n",
        "    return merged\n",
        "\n",
        "  df['merged_text'] = df['id'].apply(get_merged_text)\n",
        "  return df\n",
        "\n",
        "original_df = original_df.sort_values(by='date')\n",
        "\n",
        "original_df = build_merged_text(original_df)\n",
        "\n",
        "display(original_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e93023",
      "metadata": {
        "id": "60e93023"
      },
      "source": [
        "## Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7252cbd4-5782-4005-9397-620fbe6b11d3",
      "metadata": {
        "id": "7252cbd4-5782-4005-9397-620fbe6b11d3"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "import re\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "from indoNLP.preprocessing import pipeline, remove_html, remove_url, replace_slang, replace_word_elongation\n",
        "\n",
        "# Pre-compiled regex patterns\n",
        "USERNAME_RE = re.compile(r'@\\w+')\n",
        "RT_RE = re.compile(r'\\brt\\b', flags=re.IGNORECASE)\n",
        "HASHTAG_RE = re.compile(r'#')\n",
        "DIGIT_RE = re.compile(r'\\d+')\n",
        "WHITESPACE_RE = re.compile(r'\\s+')\n",
        "\n",
        "# Pre-compiled translation\n",
        "PUNCT_TRANSLATOR = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def fast_clean(text):\n",
        "  # Normalize Unicode (remove fancy fonts, underlines)\n",
        "  text = unidecode(text)\n",
        "\n",
        "  # Case folding to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Remove usernames\n",
        "  text = USERNAME_RE.sub('', text)\n",
        "\n",
        "  # Remove RT\n",
        "  text = RT_RE.sub('', text)\n",
        "\n",
        "  # Remove hashtag symbol but keep the word\n",
        "  text = HASHTAG_RE.sub('', text)\n",
        "\n",
        "  # Remove digits\n",
        "  text = DIGIT_RE.sub('', text)\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = text.translate(PUNCT_TRANSLATOR)\n",
        "\n",
        "  # Remove emojis\n",
        "  text = emoji.replace_emoji(text, replace='')\n",
        "\n",
        "  # Remove extra whitespace\n",
        "  text = WHITESPACE_RE.sub(' ', text).strip()\n",
        "\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9cc6cb3",
      "metadata": {
        "id": "a9cc6cb3"
      },
      "source": [
        "## Run preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1901ff96-5c36-4386-a83a-ecfb01bdc8f5",
      "metadata": {
        "id": "1901ff96-5c36-4386-a83a-ecfb01bdc8f5"
      },
      "outputs": [],
      "source": [
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize()\n",
        "\n",
        "preprocessed_df = original_df.copy()\n",
        "\n",
        "preprocessed_df.dropna(subset=[\n",
        "  'text',\n",
        "  'raw_text'\n",
        "], inplace=True)\n",
        "\n",
        "preprocessed_df['fast_clean'] = preprocessed_df['merged_text'].parallel_apply(fast_clean)\n",
        "\n",
        "indonlp_pipeline = pipeline([\n",
        "  remove_html,\n",
        "  remove_url,\n",
        "  replace_slang,\n",
        "  replace_word_elongation\n",
        "])\n",
        "\n",
        "preprocessed_df['basic_clean'] = preprocessed_df['fast_clean'].parallel_apply(indonlp_pipeline)\n",
        "\n",
        "preprocessed_df.dropna(subset=[\n",
        "  'text',\n",
        "  'raw_text',\n",
        "  'fast_clean',\n",
        "  'basic_clean'\n",
        "], inplace=True)\n",
        "\n",
        "display(preprocessed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8f8011",
      "metadata": {
        "id": "3e8f8011"
      },
      "source": [
        "## Filter out text that is shorter than 3 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478d01ee",
      "metadata": {
        "id": "478d01ee"
      },
      "outputs": [],
      "source": [
        "preprocessed_df = preprocessed_df[preprocessed_df['basic_clean'].str.split().str.len() > 3].copy()\n",
        "\n",
        "display(preprocessed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YG4xK2tLRZ1s",
      "metadata": {
        "id": "YG4xK2tLRZ1s"
      },
      "source": [
        "## Analyze data distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5MBSiYwBRaVw",
      "metadata": {
        "id": "5MBSiYwBRaVw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "original_plt = original_df.groupby(original_df['date'].dt.to_period('Q')).size()\n",
        "preprocessed_plt = preprocessed_df.groupby(preprocessed_df['date'].dt.to_period('Q')).size()\n",
        "\n",
        "original_plt.index = original_plt.index.to_timestamp()\n",
        "preprocessed_plt.index = preprocessed_plt.index.to_timestamp()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "original_plt.sort_index().plot(label='Original', marker='o')\n",
        "preprocessed_plt.sort_index().plot(label='Preprocessed', marker='s')\n",
        "\n",
        "plt.title('Yearly Message Volume Comparison')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Messages')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JP9ILVEjOy0F",
      "metadata": {
        "id": "JP9ILVEjOy0F"
      },
      "source": [
        "## Retrieving language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uksaWKy5OzOH",
      "metadata": {
        "id": "uksaWKy5OzOH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fasttext\n",
        "import urllib.request\n",
        "\n",
        "filename = 'lid.176.bin'\n",
        "model_url = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin'\n",
        "model_path = ''\n",
        "\n",
        "try:\n",
        "  model_dir = './models/fasttext'\n",
        "  if not os.path.exists(model_dir):\n",
        "    print('Creating model directory...')\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "  else:\n",
        "    print('Model directory already exists.')\n",
        "\n",
        "  model_path = os.path.join(model_dir, filename)\n",
        "\n",
        "  if not os.path.exists(model_path):\n",
        "    print('Model not found locally. Downloading...')\n",
        "    urllib.request.urlretrieve(model_url, model_path)\n",
        "    print('Download completed!')\n",
        "\n",
        "  print(f'Loading model from {model_path}...')\n",
        "  model = fasttext.load_model(model_path)\n",
        "  print('Model loaded successfully!')\n",
        "\n",
        "except Exception as e:\n",
        "  print(f'Error loading FastText model: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e04f07b",
      "metadata": {
        "id": "6e04f07b"
      },
      "source": [
        "## Identify language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45271ac8",
      "metadata": {
        "id": "45271ac8"
      },
      "outputs": [],
      "source": [
        "fasttext_df = preprocessed_df.copy()\n",
        "\n",
        "texts = fasttext_df['basic_clean'].tolist()\n",
        "\n",
        "predictions = model.predict(texts)\n",
        "\n",
        "fasttext_df['lang_detected'] = [label[0].replace('__label__', '') for label in predictions[0]]\n",
        "fasttext_df['lang_confidence'] = [float(score[0]) for score in predictions[1]]\n",
        "\n",
        "lang_stats = fasttext_df.groupby('lang_detected')['lang_confidence'].agg(\n",
        "  count='count',\n",
        "  min='min',\n",
        "  q25=lambda x: x.quantile(0.25),\n",
        "  mean='mean',\n",
        "  median='median',\n",
        "  q90=lambda x: x.quantile(0.9),\n",
        "  max='max'\n",
        ").reset_index().sort_values(by='count', ascending=False)\n",
        "\n",
        "display(lang_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b29d9de",
      "metadata": {
        "id": "3b29d9de"
      },
      "source": [
        "## Filter text with Bahasa Indonesia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf620a7",
      "metadata": {
        "id": "9bf620a7"
      },
      "outputs": [],
      "source": [
        "fasttext_df = fasttext_df[\n",
        "  (fasttext_df['lang_detected'] == 'id')\n",
        "].copy()\n",
        "\n",
        "display(fasttext_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t6QTDBnfRj3o",
      "metadata": {
        "id": "t6QTDBnfRj3o"
      },
      "source": [
        "## Analyze data distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gwNnMO7rRkQI",
      "metadata": {
        "id": "gwNnMO7rRkQI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "original_plt = original_df.groupby(original_df['date'].dt.to_period('Q')).size()\n",
        "preprocessed_plt = preprocessed_df.groupby(preprocessed_df['date'].dt.to_period('Q')).size()\n",
        "fasttext_plt = fasttext_df.groupby(fasttext_df['date'].dt.to_period('Q')).size()\n",
        "\n",
        "original_plt.index = original_plt.index.to_timestamp()\n",
        "preprocessed_plt.index = preprocessed_plt.index.to_timestamp()\n",
        "fasttext_plt.index = fasttext_plt.index.to_timestamp()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "original_plt.sort_index().plot(label='Original', marker='o')\n",
        "preprocessed_plt.sort_index().plot(label='Preprocessed', marker='s')\n",
        "fasttext_plt.sort_index().plot(label='Language Filtered', marker='^')\n",
        "\n",
        "plt.title('Yearly Message Volume Comparison')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Messages')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00eb2efe",
      "metadata": {
        "id": "00eb2efe"
      },
      "source": [
        "## Store language filtered dataframe to local file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ddc9e68",
      "metadata": {
        "id": "6ddc9e68"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "fasttext_df['date'] = fasttext_df['date'].dt.tz_localize(None)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_filename = f'language_filtered_telegram_messages_{timestamp}.xlsx'\n",
        "fasttext_df.to_excel(excel_filename, index=False)\n",
        "\n",
        "file_path = f'./data/'\n",
        "!mkdir -p $file_path\n",
        "!mv $excel_filename $file_path\n",
        "print(f'Successfully moved {excel_filename} to {file_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IOPABdhCkE2N",
      "metadata": {
        "id": "IOPABdhCkE2N"
      },
      "source": [
        "## Train POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R4ZavFpmklJr",
      "metadata": {
        "id": "R4ZavFpmklJr"
      },
      "outputs": [],
      "source": [
        "from flair.datasets import UD_INDONESIAN\n",
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "corpus = UD_INDONESIAN()\n",
        "\n",
        "tag_type = 'upos'\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "\n",
        "embedding = WordEmbeddings('id')\n",
        "\n",
        "tagger = SequenceTagger(\n",
        "  hidden_size=256,\n",
        "  embeddings=embedding,\n",
        "  tag_dictionary=tag_dictionary,\n",
        "  tag_type=tag_type,\n",
        "  use_crf=True\n",
        ")\n",
        "\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train(\n",
        "  base_path='pos-id-model',\n",
        "  learning_rate=0.1,\n",
        "  mini_batch_size=32,\n",
        "  max_epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RUBPEGk1rqhL",
      "metadata": {
        "id": "RUBPEGk1rqhL"
      },
      "source": [
        "## Store POS tagger model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K7RzXTI_rq_I",
      "metadata": {
        "id": "K7RzXTI_rq_I"
      },
      "outputs": [],
      "source": [
        "dir_name = 'pos-id-model'\n",
        "dir_source = f'./{dir_name}'\n",
        "dir_target = f'./models/{dir_name}'\n",
        "\n",
        "!mkdir -p $(dirname $dir_target)\n",
        "\n",
        "!mv $dir_source $dir_target\n",
        "\n",
        "print(f'Successfully moved {dir_name} to {dir_target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a13585c",
      "metadata": {},
      "source": [
        "## Retrieving language filtered dataframe from local file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220663a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  filename = 'language_filtered_telegram_messages_20250529_185638.xlsx'\n",
        "  file_path = f'./data/{filename}'\n",
        "\n",
        "  if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f'File {file_path} does not exist')\n",
        "  \n",
        "  print(f'Reading from {file_path} to dataframe')\n",
        "\n",
        "  fasttext_df = pd.read_excel(file_path)\n",
        "  \n",
        "  fasttext_df['date'] = pd.to_datetime(\n",
        "    fasttext_df['date'],\n",
        "    format='%Y-%m-%d %H:%M:%S',\n",
        "    errors='coerce'\n",
        "  )\n",
        "\n",
        "  print(f'Successfully read from {file_path} to dataframe')\n",
        "\n",
        "except FileNotFoundError:\n",
        "  fasttext_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "\n",
        "except Exception as e:\n",
        "  fasttext_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(fasttext_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vu9OVqQRMvsn",
      "metadata": {
        "id": "vu9OVqQRMvsn"
      },
      "source": [
        "## Predict POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w0LBmQWrMwB6",
      "metadata": {
        "id": "w0LBmQWrMwB6"
      },
      "outputs": [],
      "source": [
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "from tqdm import tqdm\n",
        "\n",
        "texts = fasttext_df['basic_clean'].tolist()\n",
        "\n",
        "tagger = SequenceTagger.load('models/pos-id-model/best-model.pt')\n",
        "\n",
        "batch_size = 10000\n",
        "pos_results = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), batch_size)):\n",
        "  batch_texts = texts[i:i + batch_size]\n",
        "  batch_sentences = [Sentence(text) for text in batch_texts]\n",
        "\n",
        "  tagger.predict(batch_sentences)\n",
        "\n",
        "  for sentence in batch_sentences:\n",
        "    tags = []\n",
        "    for token in sentence:\n",
        "      tag = token.tag if token.labels else None\n",
        "      tags.append((token.text, tag))\n",
        "    pos_results.append(tags)\n",
        "\n",
        "postag_df = fasttext_df.copy()\n",
        "\n",
        "postag_df['pos_tags'] = pos_results\n",
        "\n",
        "display(postag_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KKAaMR_lXQJy",
      "metadata": {
        "id": "KKAaMR_lXQJy"
      },
      "source": [
        "## Filter relevant POS and them stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yEi6lnfmXSO4",
      "metadata": {
        "id": "yEi6lnfmXSO4"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from functools import lru_cache\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "pandarallel.initialize()\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "@lru_cache(maxsize=10000)\n",
        "def cached_stem(word):\n",
        "  return stemmer.stem(word)\n",
        "\n",
        "allowed_tags = {'PROPN', 'NOUN', 'VERB', 'ADJ', 'ADV'}\n",
        "\n",
        "def filter_and_stem(tagged_tokens):\n",
        "  return [\n",
        "    stemmed for word, tag in tagged_tokens\n",
        "    if tag in allowed_tags and (stemmed := cached_stem(word))\n",
        "  ]\n",
        "\n",
        "postag_df['stemmed_tokens'] = postag_df['pos_tags'].parallel_apply(filter_and_stem)\n",
        "postag_df = postag_df[postag_df['stemmed_tokens'].str.len() > 0].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MN2bBFVWZmeM",
      "metadata": {
        "id": "MN2bBFVWZmeM"
      },
      "source": [
        "## Store POS tagged messsages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fPdUU5M9ZmzT",
      "metadata": {
        "id": "fPdUU5M9ZmzT"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "postag_df['date'] = postag_df['date'].dt.tz_localize(None)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "excel_filename = f'pos_tagged_telegram_messages_{timestamp}.xlsx'\n",
        "postag_df.to_excel(excel_filename, index=False)\n",
        "\n",
        "file_path = f'./data/'\n",
        "!mkdir -p $file_path\n",
        "!mv $excel_filename $file_path\n",
        "print(f'Successfully moved {excel_filename} to {file_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JCkopDQXZn6q",
      "metadata": {
        "id": "JCkopDQXZn6q"
      },
      "source": [
        "## Retrieve POS tagged and filtered messages from local file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jobIYWNuZoMX",
      "metadata": {
        "id": "jobIYWNuZoMX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  filename = 'pos_tagged_telegram_messages_20250529_215008.xlsx'\n",
        "  file_path = f'./data/{filename}'\n",
        "\n",
        "  postag_df = pd.read_excel(file_path)\n",
        "\n",
        "  print(f'Successfully read from {file_path} to dataframe')\n",
        "\n",
        "except FileNotFoundError:\n",
        "  postag_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "\n",
        "except Exception as e:\n",
        "  postag_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(postag_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3f0010",
      "metadata": {
        "id": "4c3f0010"
      },
      "source": [
        "## Run LDA topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79ee2fa",
      "metadata": {
        "id": "e79ee2fa"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from pandarallel import pandarallel\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pandarallel.initialize()\n",
        "\n",
        "messages_df = postag_df.copy()\n",
        "\n",
        "messages_df['stemmed_tokens'] = messages_df['stemmed_tokens'].parallel_apply(\n",
        "  lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        ")\n",
        "\n",
        "tokenized_docs = messages_df['stemmed_tokens'].tolist()\n",
        "\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.8)\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "filtered = [(i, doc_bow, doc) for i, (doc_bow, doc) in enumerate(zip(corpus, tokenized_docs)) if len(doc_bow) > 0]\n",
        "indices, corpus, tokenized_docs = zip(*filtered)\n",
        "\n",
        "def compute_coherence_values(dictionary, corpus, texts, start=10, limit=15, step=1):\n",
        "  coherence_values = []\n",
        "  model_list = []\n",
        "  for num_topics in range(start, limit + 1, step):\n",
        "    model = models.LdaModel(\n",
        "      corpus=corpus,\n",
        "      id2word=dictionary,\n",
        "      num_topics=num_topics,\n",
        "      random_state=42,\n",
        "      passes=10,\n",
        "      alpha='auto'\n",
        "    )\n",
        "    model_list.append(model)\n",
        "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_values.append(coherence_model.get_coherence())\n",
        "  return model_list, coherence_values\n",
        "\n",
        "start, limit, step = 10, 15, 1\n",
        "model_list, coherence_values = compute_coherence_values(dictionary, corpus, tokenized_docs, start, limit, step)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a589b2f4",
      "metadata": {},
      "source": [
        "## Visualize coherence score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07b0283",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = list(range(start, limit + 1, step))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, coherence_values)\n",
        "\n",
        "for i, (num_topics, score) in enumerate(zip(x, coherence_values)):\n",
        "  plt.text(num_topics, score, f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.xlabel('Number of Topics')\n",
        "plt.ylabel('Coherence Score (c_v)')\n",
        "plt.title('Optimal Number of Topics Based on Coherence')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51f2a940",
      "metadata": {},
      "source": [
        "## Attach topic to each message and store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499a78c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "output_dir = './data/topic_assignments'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "for num_topics, model in zip(x, model_list):\n",
        "  print(f'Assigning topics for {num_topics} topics...')\n",
        "\n",
        "  dominant_topics = []\n",
        "  topic_keywords = []\n",
        "\n",
        "  for doc_bow in corpus:\n",
        "    topic_probs = model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
        "    dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
        "    dominant_topics.append(dominant_topic)\n",
        "    topic_keywords.append(', '.join([word for word, _ in model.show_topic(dominant_topic, topn=10)]))\n",
        "\n",
        "  assigned_df = messages_df.iloc[list(indices)].copy()\n",
        "  assigned_df['assigned_topic'] = dominant_topics\n",
        "  assigned_df['topic_keywords'] = topic_keywords\n",
        "\n",
        "  print(f'Storing messages with {num_topics} topics...')\n",
        "  assigned_df.to_excel(\n",
        "    os.path.join(output_dir, f'{num_topics}_assigned_topics_telegram_messages_{timestamp}.xlsx'), \n",
        "    index=False\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8434a2d3",
      "metadata": {},
      "source": [
        "## Retrieved topic attributed messages from local file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e726bdc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  filename = '11_assigned_topics_telegram_messages_20250530_172416.xlsx'\n",
        "  file_path = f'./data/topic_assignments/{filename}'\n",
        "\n",
        "  messages_df = pd.read_excel(file_path)\n",
        "\n",
        "  print(f'Successfully read from {file_path} to dataframe')\n",
        "\n",
        "except FileNotFoundError:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'File not found')\n",
        "\n",
        "except Exception as e:\n",
        "  messages_df = pd.DataFrame()\n",
        "  print(f'Error reading file: {e}')\n",
        "\n",
        "display(messages_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de19e362",
      "metadata": {},
      "source": [
        "## List down topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2658141",
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_topic_pairs = messages_df[['assigned_topic', 'topic_keywords']].drop_duplicates()\n",
        "unique_topic_pairs = unique_topic_pairs.sort_values(by=['assigned_topic', 'topic_keywords'])\n",
        "display(unique_topic_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea86f0e",
      "metadata": {},
      "source": [
        "## Analyze sample messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe10115",
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_size = 20\n",
        "assigned_topic = 10\n",
        "\n",
        "sample_df = (\n",
        "  messages_df.groupby(['assigned_topic'], group_keys=False)\n",
        "  .apply(lambda x: x.sample(n=sample_size, random_state=42) if len(x) >= sample_size else x)\n",
        "  .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "sample_df = sample_df.sort_values(by=['assigned_topic'])\n",
        "\n",
        "display(sample_df[sample_df['assigned_topic'] == assigned_topic])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c5f5160",
      "metadata": {},
      "source": [
        "## Visualize topic distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16e7ca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "messages_df['quarter'] = messages_df['date'].dt.to_period('Q').astype(str)\n",
        "\n",
        "topic_counts = messages_df.groupby(['quarter', 'assigned_topic']).size().reset_index(name='count')\n",
        "\n",
        "topic_distribution = topic_counts.pivot(index='quarter', columns='assigned_topic', values='count').fillna(0)\n",
        "\n",
        "topic_distribution = topic_distribution.sort_index()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(topic_distribution.T, cmap='YlGnBu', linewidths=0.5, annot=True, fmt='.0f')\n",
        "plt.title('Topic Frequency by Quarter')\n",
        "plt.ylabel('Topic')\n",
        "plt.xlabel('Quarter')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GDeZjGBhA8_B",
      "metadata": {
        "id": "GDeZjGBhA8_B"
      },
      "source": [
        "## Identify knowledge roles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "q5VeeN-nA9Up",
      "metadata": {
        "id": "q5VeeN-nA9Up"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from datetime import timedelta\n",
        "from collections import Counter\n",
        "\n",
        "messages_df['assigned_topic'] = messages_df['assigned_topic'].astype('Int64')\n",
        "\n",
        "def get_quarter_range(quarter_str, buffer_days=30):\n",
        "  quarter_start = pd.Period(quarter_str, freq='Q').start_time\n",
        "  quarter_end = pd.Period(quarter_str, freq='Q').end_time\n",
        "  return quarter_start - timedelta(days=buffer_days), quarter_end\n",
        "\n",
        "def classify_user(row, indegree_threshold, betweenness_threshold):\n",
        "  if row['betweenness'] >= betweenness_threshold:\n",
        "    return 'indirect_broker'\n",
        "  elif row['in_degree'] >= indegree_threshold:\n",
        "    return 'direct_broker'\n",
        "  elif row['in_degree'] > row['out_degree']:\n",
        "    return 'seeker'\n",
        "  elif row['out_degree'] > row['in_degree']:\n",
        "    return 'initiator'\n",
        "  else:\n",
        "    return 'neutral'\n",
        "\n",
        "role_dist_results = pd.DataFrame()\n",
        "top_users_results = pd.DataFrame()\n",
        "all_user_roles_df = pd.DataFrame()\n",
        "all_topic_edges_df = pd.DataFrame()\n",
        "\n",
        "quarter_topic_pairs = (\n",
        "  messages_df.dropna(subset=['assigned_topic'])[['quarter', 'assigned_topic']]\n",
        "  .drop_duplicates()\n",
        "  .sort_values(['quarter', 'assigned_topic'])\n",
        ")\n",
        "\n",
        "for _, row in quarter_topic_pairs.iterrows():\n",
        "  quarter = row['quarter']\n",
        "  topic = row['assigned_topic']\n",
        "  if pd.isnull(quarter) or pd.isnull(topic):\n",
        "    continue\n",
        "\n",
        "  topic_keywords = (\n",
        "    messages_df[\n",
        "      (messages_df['quarter'] == quarter) &\n",
        "      (messages_df['assigned_topic'] == topic)\n",
        "    ]['topic_keywords']\n",
        "    .dropna()\n",
        "    .unique()\n",
        "  )\n",
        "  topic_keywords = ', '.join(topic_keywords) if len(topic_keywords) > 0 else ''\n",
        "\n",
        "  buffered_start, quarter_end = get_quarter_range(quarter)\n",
        "\n",
        "  subset_messages_df = messages_df[\n",
        "    (messages_df['date'] >= buffered_start) &\n",
        "    (messages_df['date'] <= quarter_end) &\n",
        "    (messages_df['assigned_topic'] == topic)\n",
        "  ].copy()\n",
        "\n",
        "  if subset_messages_df.empty:\n",
        "    continue\n",
        "\n",
        "  id_to_sender = subset_messages_df.set_index('id')['sender_id'].to_dict()\n",
        "\n",
        "  quarter_start = pd.Period(quarter, freq='Q').start_time\n",
        "\n",
        "  reply_msgs_df = subset_messages_df[\n",
        "    (subset_messages_df['date'] >= quarter_start) &\n",
        "    (subset_messages_df['date'] <= quarter_end) &\n",
        "    (subset_messages_df['reply_to_msg_id'].notna())\n",
        "  ]\n",
        "\n",
        "  topic_edge_counter = Counter()\n",
        "\n",
        "  for _, msg in reply_msgs_df.iterrows():\n",
        "    replier = msg['sender_id']\n",
        "    reply_to_id = msg['reply_to_msg_id']\n",
        "    original_sender = id_to_sender.get(reply_to_id)\n",
        "\n",
        "    if pd.notna(replier) and pd.notna(original_sender) and replier != original_sender:\n",
        "      key = (original_sender, replier, topic, quarter)\n",
        "      topic_edge_counter[key] += 1\n",
        "\n",
        "  G = nx.DiGraph()\n",
        "  topic_edges = []\n",
        "  for (original_sender, replier, topic, quarter), weight in topic_edge_counter.items():\n",
        "    G.add_edge(original_sender, replier, weight=weight)\n",
        "    topic_edges.append({\n",
        "      'quarter': quarter,\n",
        "      'assigned_topic': topic,\n",
        "      'sender_id': original_sender,\n",
        "      'replier_id': replier,\n",
        "      'weight': weight\n",
        "    })\n",
        "\n",
        "  all_topic_edges_df = pd.concat([all_topic_edges_df, pd.DataFrame(topic_edges)], ignore_index=True)\n",
        "\n",
        "  in_deg = dict(G.in_degree())\n",
        "  out_deg = dict(G.out_degree())\n",
        "  betweenness = nx.betweenness_centrality(G, normalized=True, weight='weight')\n",
        "\n",
        "  all_users = list(set(in_deg) | set(out_deg))\n",
        "\n",
        "  user_roles = pd.DataFrame({\n",
        "    'sender_id': all_users,\n",
        "    'in_degree': [in_deg.get(uid, 0) for uid in all_users],\n",
        "    'out_degree': [out_deg.get(uid, 0) for uid in all_users],\n",
        "    'betweenness': [betweenness.get(uid, 0) for uid in all_users]\n",
        "  })\n",
        "\n",
        "  indegree_threshold = user_roles['in_degree'].quantile(0.95)\n",
        "  betweenness_threshold = user_roles['betweenness'].quantile(0.95)\n",
        "\n",
        "  user_roles['role'] = user_roles.apply(\n",
        "    lambda row: classify_user(row, indegree_threshold, betweenness_threshold), axis=1\n",
        "  )\n",
        "\n",
        "  msg_counts = subset_messages_df[\n",
        "    (subset_messages_df['date'] >= quarter_start) &\n",
        "    (subset_messages_df['date'] <= quarter_end)\n",
        "  ].groupby('sender_id').size().reset_index(name='message_count')\n",
        "\n",
        "  user_roles = user_roles.merge(msg_counts, on='sender_id', how='left').fillna({'message_count': 0})\n",
        "\n",
        "  user_roles['quarter'] = quarter\n",
        "  user_roles['assigned_topic'] = topic\n",
        "\n",
        "  all_user_roles_df = pd.concat([all_user_roles_df, user_roles], ignore_index=True)\n",
        "\n",
        "  role_counts = user_roles['role'].value_counts().to_dict()\n",
        "\n",
        "  role_dist = pd.DataFrame([{\n",
        "    'quarter': quarter,\n",
        "    'assigned_topic': topic,\n",
        "    'topic_keywords': topic_keywords,\n",
        "    'indirect_broker': role_counts.get('indirect_broker', 0),\n",
        "    'direct_broker': role_counts.get('direct_broker', 0),\n",
        "    'seeker': role_counts.get('seeker', 0),\n",
        "    'initiator': role_counts.get('initiator', 0),\n",
        "    'neutral': role_counts.get('neutral', 0),\n",
        "  }])\n",
        "  \n",
        "  role_dist_results = pd.concat([role_dist_results, role_dist], ignore_index=True)\n",
        "\n",
        "  top_users = {}\n",
        "\n",
        "  role_metric = {\n",
        "    'indirect_broker': 'betweenness',\n",
        "    'direct_broker': 'in_degree',\n",
        "    'seeker': 'in_degree',\n",
        "    'initiator': 'out_degree',\n",
        "    'neutral': 'message_count'\n",
        "  }\n",
        "\n",
        "  for role in ['indirect_broker', 'direct_broker', 'seeker', 'initiator', 'neutral']:\n",
        "    metric = role_metric[role]\n",
        "    top = (\n",
        "      user_roles[user_roles['role'] == role]\n",
        "      .sort_values(metric, ascending=False)\n",
        "      .head(3)[['sender_id', metric]]\n",
        "      .to_dict('records')\n",
        "    )\n",
        "    top_users[f'top_{role}s'] = top\n",
        "\n",
        "  top_users_df = pd.DataFrame([{\n",
        "    'quarter': quarter,\n",
        "    'assigned_topic': topic,\n",
        "    'topic_keywords': topic_keywords,\n",
        "    **top_users\n",
        "  }])\n",
        "  top_users_results = pd.concat([top_users_results, top_users_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab887873",
      "metadata": {},
      "source": [
        "# Get role from user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a28cb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_ids = [\n",
        "  5191257541, \n",
        "  305620114, \n",
        "  5028290355, \n",
        "  305620114, \n",
        "  5028290355, \n",
        "  1740558998, \n",
        "  5028290355, \n",
        "  1740558998,\n",
        "]\n",
        "\n",
        "display(\n",
        "  all_user_roles_df[\n",
        "    (all_user_roles_df['sender_id'].isin(user_ids)) &\n",
        "    (all_user_roles_df['assigned_topic'] == 10)\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0aa9701",
      "metadata": {},
      "source": [
        "## Role Distribution over Quarters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85a8578",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "melted = role_dist_results.melt(\n",
        "  id_vars=['quarter', 'assigned_topic'],\n",
        "  value_vars=['indirect_broker', 'direct_broker', 'seeker', 'initiator', 'neutral'],\n",
        "  var_name='role',\n",
        "  value_name='count'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "heatmap_data = melted.pivot_table(index='role', columns='quarter', values='count', aggfunc='sum')\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='g', cmap='YlGnBu')\n",
        "plt.title('Role Distribution Across Quarters')\n",
        "plt.ylabel('Role')\n",
        "plt.xlabel('Quarter')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ea00e2",
      "metadata": {},
      "source": [
        "# Role-Topic Distribution over Quarters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7bebbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "melted = role_dist_results.melt(\n",
        "  id_vars=['quarter', 'assigned_topic'],\n",
        "  value_vars=['indirect_broker', 'direct_broker', 'seeker', 'initiator', 'neutral'],\n",
        "  var_name='role',\n",
        "  value_name='count'\n",
        ")\n",
        "\n",
        "topics = sorted(melted['assigned_topic'].dropna().unique())\n",
        "n_cols = 2\n",
        "n_rows = int(np.ceil(len(topics) / n_cols))\n",
        "\n",
        "cell_width = 6\n",
        "cell_height = 4.5\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(cell_width * n_cols, cell_height * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "max_count = melted['count'].max()\n",
        "cmap = sns.color_palette(\"YlGnBu\", as_cmap=True)\n",
        "\n",
        "for i, topic in enumerate(topics):\n",
        "  topic_data = melted[melted['assigned_topic'] == topic]\n",
        "  pivot = topic_data.pivot_table(index='role', columns='quarter', values='count', aggfunc='sum').fillna(0)\n",
        "\n",
        "  ax = axes[i]\n",
        "  sns.heatmap(\n",
        "    pivot,\n",
        "    ax=ax,\n",
        "    cmap=cmap,\n",
        "    vmin=0,\n",
        "    vmax=max_count,\n",
        "    linewidths=0.5,\n",
        "    linecolor='gray',\n",
        "    annot=True,\n",
        "    fmt=\".0f\",\n",
        "    annot_kws={'size': 8, 'rotation': 90},\n",
        "    cbar=True\n",
        "  )\n",
        "\n",
        "  ax.set_title(f'Topic {topic}', fontsize=12)\n",
        "  ax.set_xlabel('Quarter', fontsize=10)\n",
        "  ax.set_ylabel('Role', fontsize=10)\n",
        "  ax.tick_params(axis='x', labelrotation=90)\n",
        "  ax.tick_params(axis='y', labelrotation=0)\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "  axes[j].axis('off')\n",
        "\n",
        "plt.suptitle('Role Distribution by Topic and Quarter', fontsize=18, y=0.96)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65410e15",
      "metadata": {},
      "source": [
        "# Decide which topic quarter to visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415f5495",
      "metadata": {},
      "outputs": [],
      "source": [
        "role_coverage = (\n",
        "  all_user_roles_df\n",
        "  .groupby(['quarter', 'assigned_topic', 'role'])\n",
        "  .size()\n",
        "  .unstack(fill_value=0)\n",
        "  .reset_index()\n",
        ")\n",
        "\n",
        "complete_role_pairs = role_coverage[\n",
        "  (role_coverage['indirect_broker'] > 0) &\n",
        "  (role_coverage['direct_broker'] > 0) &\n",
        "  (role_coverage['seeker'] > 0) &\n",
        "  (role_coverage['initiator'] > 0) &\n",
        "  (role_coverage['neutral'] > 0)\n",
        "]\n",
        "\n",
        "user_counts = (\n",
        "  all_user_roles_df\n",
        "  .groupby(['quarter', 'assigned_topic'])['sender_id']\n",
        "  .nunique()\n",
        "  .reset_index(name='user_count')\n",
        ")\n",
        "\n",
        "complete_role_pairs = complete_role_pairs.merge(user_counts, on=['quarter', 'assigned_topic'])\n",
        "\n",
        "filtered = complete_role_pairs[\n",
        "  (complete_role_pairs['user_count'] >= 20) & \n",
        "  (complete_role_pairs['user_count'] <= 100)\n",
        "]\n",
        "\n",
        "filtered = filtered.sort_values(['user_count'])\n",
        "\n",
        "display(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75044da0",
      "metadata": {},
      "source": [
        "# Build graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff99a9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def radial_layout(G, role_dict):\n",
        "  pos = {}\n",
        "  roles_by_layer = {\n",
        "    0: ['initiator'],\n",
        "    1: ['indirect_broker', 'direct_broker'],\n",
        "    2: ['seeker']\n",
        "  }\n",
        "\n",
        "  for layer, roles in roles_by_layer.items():\n",
        "    nodes = [n for n, role in role_dict.items() if role in roles]\n",
        "    if not nodes:\n",
        "      continue\n",
        "\n",
        "    angle_step = 2 * np.pi / len(nodes)\n",
        "    radius = (layer + 1) * 2.5\n",
        "\n",
        "    for i, node in enumerate(nodes):\n",
        "      angle = i * angle_step\n",
        "      x = radius * np.cos(angle)\n",
        "      y = radius * np.sin(angle)\n",
        "      pos[node] = (x, y)\n",
        "\n",
        "  return pos\n",
        "\n",
        "selected_quarter = '2022Q3'\n",
        "selected_topic = 2\n",
        "quarter_filtered = filtered[\n",
        "  (filtered['quarter'] == selected_quarter) &\n",
        "  (filtered['assigned_topic'] == selected_topic)\n",
        "]\n",
        "sample = quarter_filtered.iloc[0]\n",
        "quarter = sample['quarter']\n",
        "topic = sample['assigned_topic']\n",
        "print(f'Selected sample: Quarter {quarter}, Topic {topic}')\n",
        "\n",
        "edges_df = all_topic_edges_df[\n",
        "  (all_topic_edges_df['quarter'] == quarter) &\n",
        "  (all_topic_edges_df['assigned_topic'] == topic)\n",
        "]\n",
        "\n",
        "roles_df = all_user_roles_df[\n",
        "  (all_user_roles_df['quarter'] == quarter) &\n",
        "  (all_user_roles_df['assigned_topic'] == topic) &\n",
        "  (all_user_roles_df['role'] != 'neutral')\n",
        "]\n",
        "\n",
        "G = nx.DiGraph()\n",
        "for _, row in edges_df.iterrows():\n",
        "  G.add_edge(row['sender_id'], row['replier_id'], weight=row['weight'])\n",
        "\n",
        "valid_nodes = set(roles_df['sender_id'])\n",
        "G = G.subgraph(valid_nodes).copy()\n",
        "\n",
        "role_dict = roles_df.set_index('sender_id')['role'].to_dict()\n",
        "nx.set_node_attributes(G, role_dict, 'role')\n",
        "\n",
        "role_colors = {\n",
        "  'indirect_broker': 'red',\n",
        "  'direct_broker': 'green',\n",
        "  'seeker': 'orange',\n",
        "  'initiator': 'skyblue'\n",
        "}\n",
        "\n",
        "node_colors = [role_colors.get(G.nodes[n].get('role'), 'black') for n in G.nodes]\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "pos = radial_layout(G, role_dict)\n",
        "\n",
        "nx.draw(\n",
        "  G, pos,\n",
        "  with_labels=True,\n",
        "  node_color=node_colors,\n",
        "  edge_color='lightgray',\n",
        "  node_size=800,\n",
        "  font_size=8\n",
        ")\n",
        "\n",
        "legend = [mpatches.Patch(color=c, label=r) for r, c in role_colors.items()]\n",
        "plt.legend(handles=legend, title='User Role', loc='best')\n",
        "plt.title(f'Knowledge Network\\nQuarter: {quarter}, Topic: {topic}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vu9OVqQRMvsn",
        "KKAaMR_lXQJy",
        "uQy9R1W4RoIT",
        "MN2bBFVWZmeM",
        "JCkopDQXZn6q",
        "IJAy8uICY01D",
        "4c3f0010",
        "FPNAB0xdjuIF",
        "GDeZjGBhA8_B",
        "HJIFcpuvqW2v"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ui-km-paper",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
